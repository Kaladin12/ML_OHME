{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2702,
     "status": "ok",
     "timestamp": 1636613605638,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "-66WsO14m06r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import inkml2img\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YH5pcWP_m061",
    "outputId": "3f63fd06-1167-432e-ff13-b47aa82464b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['expressmatch', 'extension', 'HAMEX', 'KAIST', 'MathBrush', 'MfrDB']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"    for t in arr[i]:\\n    # si la extension no es .lg\\n    if ('.lg' not in t):\\n        # se llama a la funcion inkml2img del modulo hominimo, especificando la ruta del inkml y la ruta a guardar la imagen\\n        inkml2img.inkml2img(PATH+'\\\\'+dataSets[i]+'\\\\'+t,DIR_PATH+'\\\\'+t[:-5]+'jpg')\\n        print(t)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En esta celda se convierten los archivos inkml a .jpg con ayuda del modulo inkml2img\n",
    "# PATH indica la ubicacion de los archivos .inkml\n",
    "PATH = r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\TrainINKML\"\n",
    "# DIR_PATH indica el directorio donde se guardarán las imagenes\n",
    "DIR_PATH = r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\images\"\n",
    "# se obtiene el nombre de las carpetas que contienen los archivos .inkml\n",
    "dataSets = os.listdir(PATH)\n",
    "# se genera una lista con listas vacias (la cantidad de carpetas en dataSets)\n",
    "arr = [[] for _ in range(len(dataSets))]\n",
    "print(dataSets)\n",
    "# se itera por cada carpeta de archivos .inkml\n",
    "for i in range(len(dataSets)):\n",
    "    # se agrega a la lista los archivos en determinada carpeta (la de la iteracion actual)\n",
    "    arr[i] = os.listdir(PATH+'\\\\'+dataSets[i])\n",
    "    # se itera por los archivos .inkml de la carpeta\n",
    "    for t in arr[i]:\n",
    "    # si la extension no es .lg\n",
    "    if ('.lg' not in t):\n",
    "        # se llama a la funcion inkml2img del modulo hominimo, especificando la ruta del inkml y la ruta a guardar la imagen\n",
    "        inkml2img.inkml2img(PATH+'\\\\'+dataSets[i]+'\\\\'+t,DIR_PATH+'\\\\'+t[:-5]+'jpg')\n",
    "        #print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvuvu5h_m063"
   },
   "outputs": [],
   "source": [
    "# en esta celda se obtienen los labels (el ground truth en latex) asociado a cada archivo .inkml\n",
    "# Paths indicando la direccion donde guardar y leer los archivos del conjunto de datos\n",
    "DIR_PATH = r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\TrainINKML\"\n",
    "LABELS_PATH = r\"/content/drive/My Drive/Colab Notebooks/TrainINKML/\"\n",
    "# diccionario donde el ground truth se guardara para cada archivo\n",
    "items = {}\n",
    "index = 0\n",
    "count = 0\n",
    "# se itera por la cantidad de carpetas que contienen archivos .inkml\n",
    "for t in arr:\n",
    "    # se itera por cada archivo inkml\n",
    "    for i in t:\n",
    "        if (\".lg\" not in i):\n",
    "            # se obtiene el path del archivo concatenando DIR_PATH con el nombre de la carpeta y del archivo\n",
    "            imgPath = DIR_PATH+\"\\\\\"+ dataSets[index]+ \"\\\\\"+i\n",
    "            # dado que .inkml es un tipo de XML, tiene una estructura de arbol, por lo que se utiliza \n",
    "            # la funcion ET.parse para hacer uso de esta estructura\n",
    "            tree = ET.parse(imgPath)\n",
    "            # se otbiene la raiz del archivo (primera etiqueta en el .inkml)\n",
    "            root = tree.getroot()\n",
    "            count =0\n",
    "            # se itera por cada hijo de la raiz\n",
    "            for item in root:\n",
    "                # si el hijo posee texto entonces es posible que contenga el ground truth (label) deseado\n",
    "                if item.text:\n",
    "                    # existen tres posibilidades debido a la forma en que fueron codificados los archivos en distintas carpetas\n",
    "                    # el primero es que el groud truth en latex se encuetre entre simbolos de dolar $\n",
    "                    if \"$\" in item.text:\n",
    "                        current = item.text\n",
    "                        # se añade como key del diccionario el nombre del archivo y se le asocia el ground truth en latex\n",
    "                        items[DIR_PATH+\"\\\\\"+ dataSets[index]+ \"\\\\\"+i] = current[1:-1]\n",
    "                        break\n",
    "                    # para la carpeta MathBrush cuyo formato de groundTruth es distinto\n",
    "                    if \"\\\\\" in item.text and dataSets[index] == \"MathBrush\":\n",
    "                        current = item.text\n",
    "                        items[DIR_PATH+\"\\\\\"+ dataSets[index]+ \"\\\\\"+i] = current[1:-1]\n",
    "                        break\n",
    "                    # para la carpeta KAIST el latex se encuentra sin elementos externos, por lo que se agrega sin mas\n",
    "                    if (count ==1 and dataSets[index]==\"KAIST\" and len(item.text)>2):\n",
    "                        current = item.text\n",
    "                        items[DIR_PATH+\"\\\\\"+ dataSets[index]+ \"\\\\\"+i] = current\n",
    "                        break\n",
    "                count +=1\n",
    "    index +=1 \n",
    "# se verifica que el comando log no se encuentre como una secuencia de letras separadas\n",
    "for key in items:\n",
    "    if ('l o g' in items[key]):\n",
    "        # si se encuentra se reemplaza por su respectivo comando en latex correcto\n",
    "        items[key] = items[key].replace('l o g', '\\\\log')\n",
    "# se guarda el diccionario en formato json\n",
    "with open(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\labels.json\", 'w') as f:\n",
    "    json.dump(items, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_6f1juom065"
   },
   "outputs": [],
   "source": [
    "# se realiza un filtrado de los groud truth para separar cada comando, letra y numero presente en el dataset\n",
    "# establece los comandos a borrar, ya que no proporcionan informacion relevante para la ecuacion\n",
    "delete = ['\\\\Bigg','\\\\left','\\\\right','\\\\Big','\\\\mathrm']\n",
    "# comandos a reemplazar por el comando latex correcto\n",
    "replace = {'\\\\to':'\\\\rightarrow', '\\\\gt':'>', '\\\\lt':'<'}\n",
    "# simbolos a los que se les añade espacios en blanco antes y despues para un mejor tratamiento\n",
    "add = ['_','{','}','=','(',')','-','+','^','[',']', ',']\n",
    "count = 0\n",
    "# se iteran los archivos .inkml, items representa el mismo diccionario guardado en labels.json\n",
    "for key in items:\n",
    "    # si existe alguno de los comandos descritos en las listas y diccionario anteriores se realizan la soperaciones\n",
    "    # correspondientes\n",
    "    for dele in delete:\n",
    "        if dele in items[key]:\n",
    "            items[key] = items[key].replace(dele, \"\")\n",
    "    for rep in replace:\n",
    "        if (rep in items[key]):\n",
    "            items[key] = items[key].replace(rep, replace[rep])\n",
    "    for it in add:\n",
    "        if (it in items[key]):\n",
    "            items[key] = items[key].replace(it, \" \"+it+\" \")\n",
    "    # se agrega un espacio en blanco antes de \\\\ para que pueda diferenciarse a los comandos del resto de letras o numeros\n",
    "    if ('\\\\' in items[key]):\n",
    "        items[key] = items[key].replace('\\\\', \" \\\\\")\n",
    "    # se separa el ground truth, siendo la condicion para separa el que existan uno o mas espacios en blanco entre caracteres\n",
    "    items[key] = re.split(r'\\s+', items[key])\n",
    "    count = 0\n",
    "    # se itera sobre la lista generada de letras, numeros, simbolos y comandos para cada archivo inkml\n",
    "    # separando los caracteres que se encuentren juntos y no sean parte de un comando, \n",
    "    # por ejemplo 'abc' se separa en 'a', 'b', 'c'\n",
    "    for a in items[key]:\n",
    "        if ('\\\\' not in a and len(a)>1):\n",
    "            uno = items[key][:count]\n",
    "            dos = re.split('', a)[1:-1]\n",
    "            tres = items[key][count+1:]\n",
    "            count+= len(dos)-1\n",
    "            uno.extend(dos)\n",
    "            uno.extend(tres)\n",
    "            items[key] = uno\n",
    "        count+=1\n",
    "    if (items[key][-1]==\"\"):\n",
    "        items[key] = items[key][:-1]\n",
    "    if (items[key][0]==\"\"):\n",
    "        items[key] = items[key][1:]\n",
    "# se guarda en labels.json\n",
    "with open(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\labels.json\", 'w') as f:\n",
    "    json.dump(items, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4394,
     "status": "ok",
     "timestamp": 1636613612533,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "EeL-Ufr0m066",
    "outputId": "3d802666-c333-4f39-c9e0-f8223f78c794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> S = ( \\sum _ { i = 1 } ^ { n } \\theta _ i - ( n - 2 ) \\pi ) r ^ 2 <end>\n"
     ]
    }
   ],
   "source": [
    "# se define el numero maximo de palabras a tokenizar\n",
    "num_words = 1000\n",
    "# token para labels desconocidas\n",
    "oov_token = '<UNK>'\n",
    "# parametros de inicializacion del tokenizer de tensorflow\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "items = {}\n",
    "# se obtienen las listas de labels (numeros, comandos latex, simbolos y letras) para cada archivo inkml\n",
    "# derivados del preprocesamiento previo\n",
    "with open(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\labels.json\", 'r') as f:\n",
    "    items = json.load(f)\n",
    "tokens = {}\n",
    "count = 0\n",
    "# se itera para cada archivo\n",
    "for key in items:\n",
    "    # se convierte a string\n",
    "    items[key] = ' '.join(map(str, items[key]))\n",
    "    # se anade al inicio y final los labels start y end para indicar inicio y final del ground truth\n",
    "    items[key] = '<start> '+items[key] + ' <end>'\n",
    "    # se vuelven a separar\n",
    "    items[key] = re.sub(r\"\\s+\", \" \", items[key])\n",
    "# se obtiene la cantidad de archvios inkml a tratar\n",
    "keys = list(items.keys())\n",
    "# se anade el groud truth de cada archivo a una lista\n",
    "data = [ items[key] for key in keys ]\n",
    "print(data[0])\n",
    "# se instancia la funcion tokenizer con los parametros establecidos en un principio\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token, filters='', lower=False)\n",
    "# se obtienen los tokens para el conjunto de datos, es decir, a cada comando de latex, letra, numero o simbolo\n",
    "# utilizado se le asigna un numero entero en base a su frecuencia de aparicion\n",
    "tokenizer.fit_on_texts(data)\n",
    "# se define la cantidad de palabras o tokens\n",
    "word_index = tokenizer.word_index\n",
    "# se guardan los tokens en tokens.json\n",
    "with open(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\tokens.json\", 'r') as f:\n",
    "        word_index = json.load(f)\n",
    "tokenizer.word_index = word_index\n",
    "# se convierten los labels a tokens para cada archivo inkml\n",
    "# por ejemplo ['a','b','c'] se convierte a [1,2,3] asumiendo que estos son sus tokens\n",
    "train_sequences = tokenizer.texts_to_sequences(data)\n",
    "# se obtiene la longitud del label mas grande\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "# se asocian los tokens a cada archivo haciendo un pad hacia la maxima longitud\n",
    "# tal que todos los archivos tenga por label una lista de la misma longitud, rellenando con ceros\n",
    "# aquellos tokens que se encuentran en una longitud mayor al verdadero para determinado archivo\n",
    "train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnyncC_8m067"
   },
   "outputs": [],
   "source": [
    "# se cargan los datos generados anteriormente: los tokens y etiquetas sin toquenizar de los\n",
    "# archivos del dataset\n",
    "LABELS_PATH = r\"/content/drive/My Drive/Colab Notebooks/TrainINKML\"\n",
    "tokens = {}\n",
    "files_imgs = {}\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/TrainINKML/tokens.json\", 'r') as f:\n",
    "    tokens = json.load(f)\n",
    "tokensPerFile = {}\n",
    "items = {}\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/TrainINKML/labels.json\", 'r') as f:\n",
    "    items = json.load(f)\n",
    "# se guardan los tokens por archivo inkml haciendo referencia ahora a la imagen \n",
    "# generada a partir del archvio inkml\n",
    "for key in items:\n",
    "    newKey = LABELS_PATH +'/images/'+ key[len(LABELS_PATH)+15:][:-6].replace('\\\\','/')+'.jpg'\n",
    "    tokensPerFile[newKey] = []\n",
    "    files_imgs[newKey] = LABELS_PATH + key[43:].replace('\\\\','/')\n",
    "    for command in items[key]:\n",
    "        tokensPerFile[newKey].append(tokens[command])\n",
    "# se guarda el conjunto de datos tokenizado en un formato json para su posterior uso\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/TrainINKML/labelsPerFile.json\", 'w') as f:\n",
    "    json.dump(tokensPerFile, f, indent=4)\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/TrainINKML/files_img.json\", 'w') as f:\n",
    "    json.dump(files_imgs, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6884,
     "status": "ok",
     "timestamp": 1636613627037,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "ALcPac7Rm067",
    "outputId": "c8424e1c-96bf-45d2-9365-0782d58dcbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 180, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 148, 178, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 148, 178, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 148, 178, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 146, 176, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 146, 176, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 146, 176, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 144, 174, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 144, 174, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 144, 174, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 142, 172, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 142, 172, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 142, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 71, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 69, 84, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 69, 84, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 69, 84, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 67, 82, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 67, 82, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 67, 82, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 65, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 65, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 65, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 63, 78, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 63, 78, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 63, 78, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 39, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 29, 37, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 29, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 29, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 27, 35, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 27, 35, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 27, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 25, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 25, 33, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 25, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 23, 31, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 23, 31, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 23, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 9, 13, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 9, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 9, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 7, 11, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 7, 11, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 7, 11, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 5, 9, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 5, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 3, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 3, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 3, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 3, 128)         0         \n",
      "=================================================================\n",
      "Total params: 826,272\n",
      "Trainable params: 823,968\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# se define la arquitectura del encoder en base a Zhang (2017)\n",
    "class FCN_encoder(tf.keras.Model):\n",
    "    def __init__(self, dropout_rate = 0.2):\n",
    "        super(FCN_encoder, self).__init__()\n",
    "        # super dentro del constructor permite que la clase herede y se convierta en un objeto de Keras\n",
    "        \n",
    "        # bloque de convolucion 1, 32 filtros\n",
    "        # cada capa convolucional es seguida por un batch normalization y una capa de activacion\n",
    "        # relu, el movimiento del kernel de convolucion se establece como 1 y\n",
    "        # la dimension del kernel se establece como 3x3\n",
    "        self.conv_1_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)\n",
    "        self.batch_1_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_1_1 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_1_2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)\n",
    "        self.batch_1_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_1_2 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_1_3 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)\n",
    "        self.batch_1_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_1_3 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_1_4 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)\n",
    "        self.batch_1_4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_1_4 = tf.keras.layers.Activation('relu')\n",
    "        \n",
    "        # maxpooling para reducir el tamaño\n",
    "        self.maxPool_1 = tf.keras.layers.MaxPooling2D()\n",
    "        \n",
    "        # bloque convolucional 2, 64 filtros\n",
    "        self.conv_2_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_2_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_2_1 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_2_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_2_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_2_2 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_2_3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_2_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_2_3 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_2_4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_2_4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_2_4 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        self.maxPool_2 = tf.keras.layers.MaxPooling2D()\n",
    "\n",
    "        self.conv_3_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_3_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_3_1 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_3_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_3_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_3_2 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_3_3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_3_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_3_3 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_3_4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_3_4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_3_4 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        self.maxPool_3 = tf.keras.layers.MaxPooling2D()\n",
    "\n",
    "        self.conv_4_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1)\n",
    "        self.batch_4_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_4_1 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_4_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1)\n",
    "        self.batch_4_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_4_2 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_4_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1)\n",
    "        self.batch_4_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_4_3 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_4_4 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1)\n",
    "        self.drop_4 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.batch_4_4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_4_4 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        self.maxPool_4 = tf.keras.layers.MaxPooling2D()\n",
    "    # funcion de llamada de la clase, dentro de la cual se establece la secuencia de layers que el inpput seguirá\n",
    "    # cada 'bloque' de codigo representa al conjunto de layers que comparten la misma cantidad de filtros en el\n",
    "    # kernel de las capas convolucionales\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv_1_1(inputs)\n",
    "        x = self.batch_1_1(x)\n",
    "        x = self.act_1_1(x)\n",
    "        x = self.conv_1_2(x)\n",
    "        x = self.batch_1_2(x)\n",
    "        x = self.act_1_2(x)\n",
    "        x = self.conv_1_3(x)\n",
    "        x = self.batch_1_3(x)\n",
    "        x = self.act_1_3(x)\n",
    "        x = self.conv_1_4(x)\n",
    "        x = self.batch_1_4(x)\n",
    "        x = self.act_1_4(x)\n",
    "        x = self.maxPool_1(x)\n",
    "        \n",
    "        x = self.conv_2_1(x)\n",
    "        x = self.batch_2_1(x)\n",
    "        x = self.act_2_1(x)\n",
    "        x = self.conv_2_2(x)\n",
    "        x = self.batch_2_2(x)\n",
    "        x = self.act_2_2(x)\n",
    "        x = self.conv_2_3(x)\n",
    "        x = self.batch_2_3(x)\n",
    "        x = self.act_2_3(x)\n",
    "        x = self.conv_2_4(x)\n",
    "        x = self.batch_2_4(x)\n",
    "        x = self.act_2_4(x)\n",
    "        x = self.maxPool_2(x)\n",
    "        \n",
    "        x = self.conv_3_1(x)\n",
    "        x = self.batch_3_1(x)\n",
    "        x = self.act_3_1(x)\n",
    "        x = self.conv_3_2(x)\n",
    "        x = self.batch_3_2(x)\n",
    "        x = self.act_3_2(x)\n",
    "        x = self.conv_3_3(x)\n",
    "        x = self.batch_3_3(x)\n",
    "        x = self.act_3_3(x)\n",
    "        x = self.conv_3_4(x)\n",
    "        x = self.batch_3_4(x)\n",
    "        x = self.act_3_4(x)\n",
    "        x = self.maxPool_3(x)\n",
    "        \n",
    "        x = self.conv_4_1(x)\n",
    "        x = self.batch_4_1(x)\n",
    "        x = self.act_4_1(x)\n",
    "        x = self.conv_4_2(x)\n",
    "        x = self.batch_4_2(x)\n",
    "        x = self.act_4_2(x)\n",
    "        x = self.conv_4_3(x)\n",
    "        x = self.batch_4_3(x)\n",
    "        x = self.act_4_3(x)\n",
    "        x = self.conv_4_4(x)\n",
    "        x = self.drop_4(x)\n",
    "        x = self.batch_4_4(x)\n",
    "        x = self.act_4_4(x)\n",
    "        x = self.maxPool_4(x)\n",
    "        # el encoder retorna el feature map generado par la ultima capa de max pooling\n",
    "        return x\n",
    "    # se instancia una clase que permite conocer el tamaño de los parametros dentro de la red.\n",
    "    def model(self):\n",
    "        input = tf.keras.layers.Input(shape=(150, 180, 1))\n",
    "        return tf.keras.Model(inputs = input, outputs = self.call(input) )\n",
    "print(FCN_encoder().model().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1636613631367,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "rBdEvEt6m068"
   },
   "outputs": [],
   "source": [
    "# el modelo de atencion es instanciado\n",
    "class Attender(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Attender, self).__init__()\n",
    "        # se instancia los distintos dense layers parametrizados en Zhang\n",
    "        \n",
    "        # representa la capa que se utiliza para aprender del hidden state anterior del decoder\n",
    "        self.W_1 = tf.keras.layers.Dense(128)\n",
    "        # capa que aprendera del feature map generado por el encoder\n",
    "        self.U_a = tf.keras.layers.Dense(128)\n",
    "        # la dimension de atencion es 128\n",
    "        self.V_a = tf.keras.layers.Dense(128)\n",
    "    \n",
    "    def call(self, a, h):\n",
    "        # se espanden dimensiones para el hidden state\n",
    "        h_t = tf.expand_dims(h, 1)\n",
    "        # se calcula el estado intermedio llamando a los dense layers y la funcion de tangente \n",
    "        # hiperbolica con la suma de los dos dense layers que reciben como parametro el hidden state\n",
    "        # y el vector de anotacion\n",
    "        e_ti = self.V_a( (tf.nn.tanh( self.W_1(h_t) + self.U_a(a))))\n",
    "        # se aplica la activacion softmax al resultado\n",
    "        a_ti = tf.nn.softmax(e_ti)\n",
    "        # se calcula el vector de contexto multiplicando los coeficientes a_ti por el vector de anotacion\n",
    "        context = a_ti * a\n",
    "        # se obtiene la suma del resutado de la multiplicacion anterior\n",
    "        context = (tf.reduce_sum(tf.reduce_sum(context, axis =1), axis=1))\n",
    "        # se regresa el vector de contexto y los coeficientes a_ti\n",
    "        return context     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1636613633565,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "3j_uVk59m069"
   },
   "outputs": [],
   "source": [
    "# se define el modelo decoder que utiliza el gru\n",
    "class GRU_decoder(tf.keras.Model):\n",
    "    def __init__(self, dimension, units, label_len):\n",
    "        super(GRU_decoder, self).__init__()\n",
    "        # las unidades del gru\n",
    "        self.units = units\n",
    "        # la capa de embedding, establecida en Zhang como E\n",
    "        self.embedding = tf.keras.layers.Embedding(label_len, dimension)\n",
    "        # la capa del gru, se especifica que se desea el regreso de las secuencias y el ultimo estado calculado\n",
    "        self.gru = tf.keras.layers.GRU( self.units, return_sequences=True, return_state = True, recurrent_initializer='glorot_uniform')\n",
    "        # dense layers para el calculo de las probabilidades para cada palabra en el diccionario de palabras\n",
    "        # es decir, los tokens\n",
    "        # dense layer utilizado para la salida del GRU\n",
    "        self.fc1 = tf.keras.layers.Dense(128)\n",
    "        # dense layer utilizado para aprender del vector de contexto generado\n",
    "        self.fc2 = tf.keras.layers.Dense(128)\n",
    "        # dense layer utilizado para aprender de la salida anterior del decoder\n",
    "        self.fc3 = tf.keras.layers.Dense(128)\n",
    "        # la ultima capa se define con una activacion softmax\n",
    "        self.fc4 = tf.keras.layers.Dense(128, activation='softmax')\n",
    "        # se instancia el modelo de atencion\n",
    "        self.attention = Attender()\n",
    "      \n",
    "    def call(self, a,x, h):\n",
    "        # la llamada recibe la salida del encoder, la entrada del decoder (su salida anterios) \n",
    "        # y el estado anterior del decoder\n",
    "        # se llama al modelo de atencion y se recibe el vector de contexto y los coeficientes de atencion\n",
    "        context_v = self.attention(a,h)\n",
    "        x = self.embedding(x)\n",
    "        # se anade el contexto a la entrada del decoder para utilizar el conocimeinto previo\n",
    "        t = tf.concat([context_v,x], axis =-1)\n",
    "        # se expanden las dimensiones para hacer compatible el tensor con la entrada del GRU\n",
    "        t = tf.expand_dims(t, 1)\n",
    "        # se alimenta al gru con dicha informacion y se obtiene una salida y su estado actual\n",
    "        out, state = self.gru(t)\n",
    "        # se reduce la dimension de la salida del gru para hacer posible su \n",
    "        out = tf.reduce_sum(out, axis=1)\n",
    "        \n",
    "        # es necesario calcular las probabilidades condicionales, por lo que se utiliza \n",
    "        # la metodologia descrita por zhang al sumar las salidas de las capas \n",
    "        # que usan la informacion de distintias secciones del modelo encoder-decoder\n",
    "        # como entrada de una capa final en la cual se realiza la clasficacion\n",
    "        a = self.fc1(out)\n",
    "        b = self.fc2(context_v)\n",
    "        c = self.fc3(x)\n",
    "        # se suman los resultados de cada capa densa\n",
    "        res = a+b+c\n",
    "        # se obtienen las probabilidades gracias al softmax del ultimo layer\n",
    "        res = self.fc4(res)\n",
    "        # se regresan las probabilidades, el estado y los coeficientes de atencion\n",
    "        # las probabilidades corresponden a un tensor que posee las siguientes dimensiones:\n",
    "        # (tamano_batch, cantidad de palabras) donde la cantidad de palabras es la misma\n",
    "        # que los tokens producidos durante el preprocesamiento.\n",
    "        return res, state\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    # funcion especial para inicializar la entrada del GRU por primera vez como un tensor\n",
    "    # de ceros, establecido como la forma mas eficiente de inicializar este t\n",
    "    def reset(self, batch):\n",
    "        return tf.zeros((batch,3, self.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definicion de funciones para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1636613659199,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "IUTMherym06-"
   },
   "outputs": [],
   "source": [
    "# instanciacion del encoder\n",
    "encoder = FCN_encoder()\n",
    "# isntanciacion del decoder, con valores para la dimension del embedding, del gru y la cantidad de tokens\n",
    "decoder = GRU_decoder(128,128,len(list(word_index.keys()))+1)\n",
    "# se define una funcion de optimizacion para el modelo, el cual permite realizar la optimizacion en base\n",
    "# al gradiente de los errores\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# definimos el tipo de calculo para los errores o perdiada entre los correspondientes labels del ground \n",
    "#truth de cada imagen y las predicciones realizadas por el modelo\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define una fucnion de loss (perdida o error) que recibe como parametro un label del ground truth por imagen \n",
    "# en el batch las matrices de prediccion asociadas a cada una de esas imagenes\n",
    "def loss_function(real, pred):\n",
    "    # se realiza una conversion del tipo de dato para las probabilidades de las predicciones\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    # se establece una mascara para aquellas imagenes cuyo label sea 0, es decir, que\n",
    "    # es el padding agregado y que de esta forma no se considere su error para el error general\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # se calcula el error\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # se anade la mascara al los obtenido\n",
    "    loss_ *= mask\n",
    "    # se retorna la media de las perdidas para las imagenes en el batch\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define una funcion para el calculo de presicion debido a que el padding en el groundTruth de las\n",
    "# imagenes implica que los ceros utilizados para ello seran tomados en cuenta durante su calculo\n",
    "# con la funcion propia de tensorflow\n",
    "def accuracy(groundTruth, pred):\n",
    "    # recibe los labels correctos del batch y las predicciones para cada una de las clases (127)\n",
    "    # se obtiene el argmax o el mayor valor de cada prediccion\n",
    "    pred_values = K.cast(K.argmax(pred, 1), dtype='int32')\n",
    "    # se establece cuales de dichas predicciones son verdaderas con base al groundTruth o etiqueta\n",
    "    correct = K.cast(K.equal(groundTruth, pred_values), dtype='float32')\n",
    "    # se utiliza una mascara para establecer aquellos espacios dentro de la etiqueta que representan un\n",
    "    # cero utilizado para el padding\n",
    "    mask = K.cast(K.greater(groundTruth, 0), dtype='float32')\n",
    "    # se toman en cuenta solamente aquellos valores que no sean cero\n",
    "    n_correct = K.sum(mask * correct)\n",
    "    # se calcula la cantidad de etiquetas del groundTruth que no son padding\n",
    "    n_total = K.sum(mask)\n",
    "      # se retorna el ratio representando la fraccion de predicciones correctas para esta seccion del batch\n",
    "    return n_correct / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2840,
     "status": "ok",
     "timestamp": 1636613665443,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "4sfEL4Psm06_"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "# se obtienen los tokens por imagen\n",
    "LABELS_PATH = r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\labelsPerFile.json\"\n",
    "with open(LABELS_PATH,  'r') as f:\n",
    "    data = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26336,
     "status": "ok",
     "timestamp": 1636613693505,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "qz69MKV0m07A",
    "outputId": "e23cfbfb-f6b7-4cff-d5cf-09646d3bd7a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (150, 180, 1)\n",
      "Label:  [ 4 23  2 10 22 27  3 15 23  2 10  3 23  2 22  3 23  2 27  3  5  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "files_img = {}\n",
    "# se obtienen los nombres de las imagenes\n",
    "with open(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\files_img.json\", 'r') as f:\n",
    "    files_img = json.load(f)\n",
    "    f.close()\n",
    "keys = list(files_img.keys())\n",
    "files = list(data.keys())\n",
    "# se obtiene el dataset a partir del diccionario que contiene los tokens por archivo\n",
    "list_ds = tf.data.Dataset.list_files(files)\n",
    "\n",
    "# funcion que obtiene la imagen y su label, tiene como parametro el path de la imagen\n",
    "def getImages(f):\n",
    "    # se obtiene el indice del archivo para obtener sus labels (tokens)\n",
    "    index = keys.index(f)\n",
    "    # se lee la imagen desde su fuente\n",
    "    image = tf.io.read_file(f)\n",
    "    # se decodifica el formato jpg y se establece como imagen en un solo canal, es decir\n",
    "    # blanco y negro\n",
    "    image = tf.image.decode_jpeg(image, channels = 1)  \n",
    "    # se convierte su tipo de dato a flotante\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # se redimensiona para que concuerde con el input del modelo (encoder)\n",
    "    image = tf.image.resize(image, [150, 180] ) \n",
    "    # se retorna la imagen (su matriz de pixels) y su label\n",
    "    return image, train_padded[index]\n",
    "\n",
    "# instancia el dataset a utilizar como un mapeo de los archvios dentro de list_ds que manda llamar a la funcion getProcessedImages\n",
    "labeledDataset = list_ds.map(lambda x: tf.py_function(getImages, [x], [tf.float32, tf.int32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# solo para verificar que funciona, imprime las dimensioens de una imagen del dataset y su label\n",
    "for image, label in labeledDataset.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1636613699338,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "lKAOv3vQm07B",
    "outputId": "8ecbe86f-2946-4942-c923-b905a17032ae"
   },
   "outputs": [],
   "source": [
    "# se utiliza un shuffle para revolver las imagenes dentro del dataset y evitar\n",
    "# que imagenes continuas siempre sean tomadas dentro del mismo batch\n",
    "labeledDataset = labeledDataset.shuffle(buffer_size=20)\n",
    "# se establece el tamano de batch como 8, es decir, se ingresara al entrenamiento paquetes de 8 imagenes\n",
    "labeledDataset = labeledDataset.batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1636613701200,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "zqvyQ5XXm07C"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# se define la funcion que entrena al modelo de encoder-decoder\n",
    "# recibe como parametros el batch de imagenes y su ground truth (labels)\n",
    "def train(image, groundTruth):\n",
    "    loss = 0\n",
    "    acc = []\n",
    "    # el estado del gru se inicializa como ceros\n",
    "    hidden = decoder.reset(groundTruth.shape[0])\n",
    "    # el input del decoder se inicializa como un tensor con valores para el primer token en todas las imagenes\n",
    "    input_decoder = tf.constant([word_index['<start>']] * groundTruth.shape[0])\n",
    "    g1 = tf.random.Generator.from_seed(1)\n",
    "    #input_decoder = tf.expand_dims([[word_index['<start>']]*3] * groundTruth.shape[0], 1)\n",
    "    #print(input_decoder.shape, hidden.shape)\n",
    "    # ciclo que permite el entrenamiento al generar un entorno donde las variables de entrenamiento son 'vigiladas'\n",
    "    # durante el entrenamiento para poder corregirlas y ajusstar el modelo\n",
    "    count = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # se llama al encoder con la imagen la cual regresa el feature map con las caracteristicas extraidas de la imagen\n",
    "        feature_map = encoder(image)\n",
    "        # se itera por la cantidad de posibles palabras en un groundTruth\n",
    "        for i in range(1, groundTruth.shape[1]):\n",
    "            # para cada palabra se verifica si su tensor corresponde a solo valores en cero, para lo cual se rompe el ciclo\n",
    "            # terminando el entrenamiento de este batch\n",
    "            sum_ = tf.reduce_sum(groundTruth[:,i])\n",
    "            allZero = tf.equal(sum_, 0)\n",
    "            if (allZero):\n",
    "                break\n",
    "            # si existe por lo menos un label por predecir entonces se llama al decoder envindole \n",
    "            # su respectivo input, las salidas del encoder y el hidden state anterior\n",
    "            pred, hidden = decoder(feature_map,input_decoder, hidden)\n",
    "            # se calcula el error de las predicciones y se suma al actual para el batch\n",
    "            loss += loss_function(groundTruth[:, i], pred)\n",
    "            acc.append(accuracy(groundTruth[:, i], pred).numpy())\n",
    "            #print(i,\"loss:\", loss)\n",
    "            # la siguiente entrada para el decoder son las anotaciones para la iteracion actual\n",
    "            # es decir, la anterior para la siguiente interacion (h-1)\n",
    "            input_decoder = groundTruth[:, i]\n",
    "            count +=1\n",
    "    # se obtienen las variables a las que se les puede modificar los parametros para ajustarlas (entrenarlas)\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # se obtiene el gradiente de error en base al error obtenido\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    # se realiza el ajuste en base al gradiente de error\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    # una vez que termina la entrada del batch se calcula el error total con el error obtenido\n",
    "    total_loss = (loss / count)\n",
    "    # se regresa el error dividido entre el numero de iteraciones para el batch,\n",
    "    # el error total del batch (loss), asi como la media de la presicion en las predicciones del batch\n",
    "    return loss, total_loss, sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1636613704474,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "VT-y7pKJm07C"
   },
   "outputs": [],
   "source": [
    "# funcion que itera sobre el dataset para entrenarlo\n",
    "def trainData(dataset, loadEpoch, batchToLoad, load, EPOCHS = 2):\n",
    "    print(len(dataset))\n",
    "    # se itera por las epocas de entrenamiento\n",
    "    for epoch in range(EPOCHS):\n",
    "        # se inicializa la perdida como 0 para cada epoca de entrenamiento\n",
    "        total_loss = 0\n",
    "        num = 0\n",
    "        # se hace iterable el dataset para obtener cada batch de images de forma individual\n",
    "        # obteniendo el numero de batch, el tensor de imagenes (pixeles) y las etiquetas de cada imagen del batch\n",
    "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "            # se llama a la funcion de entrenamiento para cada batch enviando el tensor de imagenes y de labels\n",
    "            # obteniendo los erroes\n",
    "            print(\"batch\",batch+1, end=\" \")\n",
    "            # dado que el ultimo batch tiene longitud distinta, se omite en cada epoca\n",
    "            if (batch+1 == 1023):\n",
    "                break\n",
    "            # se llama a la funcion de entrenamiento, enviando como parametro el batch de imagenes y labels\n",
    "            batch_loss, t_loss, acc = train(img_tensor, target)\n",
    "            \n",
    "            if (load):\n",
    "                break\n",
    "            # utilizado para guardar los pesos del encoder y decoder cada 100 batches\n",
    "            if ((batch+1)%100 == 0 ):\n",
    "                encoder.save_weights(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\train4\\enc\\{0}_{1}.h5\".format(loadEpoch+1+epoch, batch),\n",
    "                    save_format='h5')\n",
    "                decoder.save_weights(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\train4\\dec\\{0}_{1}.h5\".format(loadEpoch+1+epoch, batch),\n",
    "                    save_format='h5')\n",
    "            print(\"loss:\", t_loss,\"accuracy:\",acc , end=\"\\n\")\n",
    "            # se suma la perdida obtenida en cada batch\n",
    "            total_loss += t_loss\n",
    "            num +=1\n",
    "        if not load:    \n",
    "            print('Epoch {0:d}/{1:d}'.format(epoch+1, EPOCHS))\n",
    "            print('===============>  train-loss=%.3f' % (total_loss/num))\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y84oT5Ozm07D",
    "outputId": "fcb9c47e-a913-4b1e-d263-dd1c6305ce97",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1023\n",
      "batch 1 1023\n",
      "batch 1 loss: tf.Tensor(4.3818984, shape=(), dtype=float32) accuracy: 0.5008365556194976\n",
      "batch 2 loss: tf.Tensor(4.431326, shape=(), dtype=float32) accuracy: 0.5304357651269661\n",
      "batch 3 loss: tf.Tensor(4.109344, shape=(), dtype=float32) accuracy: 0.46498599823783426\n",
      "batch 4 loss: tf.Tensor(3.4599683, shape=(), dtype=float32) accuracy: 0.5295995704152368\n",
      "batch 5 loss: tf.Tensor(2.5593164, shape=(), dtype=float32) accuracy: 0.38543651103973386\n",
      "batch 6 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18464/2681839606.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# se llama a la funcion de entrenamiento para entrenar haciendo uso de los pesos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# obtenidos de los entrenamientos previos, continuando el aprendizaje\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrainData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeledDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m999\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18464/1418600270.py\u001b[0m in \u001b[0;36mtrainData\u001b[1;34m(dataset, loadEpoch, batchToLoad, load, EPOCHS)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# se llama a la funcion de entrenamiento, enviando como parametro el batch de imagenes y labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18464/566123584.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(image, groundTruth)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# si existe por lo menos un label por predecir entonces se llama al decoder envindole\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# su respectivo input, las salidas del encoder y el hidden state anterior\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;31m# se calcula el error de las predicciones y se suma al actual para el batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroundTruth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18464/3479804994.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, a, x, h)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# se obtienen las probabilidades gracias al softmax del ultimo layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m# se regresan las probabilidades, el estado y los coeficientes de atencion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# las probabilidades corresponden a un tensor que posee las siguientes dimensiones:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1240\u001b[0m             self.kernel, ids, weights, combiner='sum')\n\u001b[0;32m   1241\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMatMul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1243\u001b[0m     \u001b[1;31m# Broadcast kernel to inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[1;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[1;32m--> 404\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5692\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5693\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5694\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   5695\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5696\u001b[0m         transpose_b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# se llama la funcion principal de entrenamiento para inicializar\n",
    "trainData(labeledDataset, 7, 999, True, 5)\n",
    "# se cargan los pesos de un entrenamiento anterior\n",
    "encoder.load_weights(r\"F:\\MEGA\\CETYS\\fünf\\Machine Learning\\enc.h5\")\n",
    "decoder.load_weights(r\"F:\\MEGA\\CETYS\\fünf\\Machine Learning\\dec.h5\")\n",
    "# se llama a la funcion de entrenamiento para entrenar haciendo uso de los pesos\n",
    "# obtenidos de los entrenamientos previos, continuando el aprendizaje\n",
    "trainData(labeledDataset, 3, 999, False, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define el numero maximo de palabras a tokenizar\n",
    "num_words = 1000\n",
    "# token para labels desconocidas\n",
    "oov_token = '<UNK>'\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "items = {}\n",
    "# se obtienen las listas de labels (numeros, comandos latex, simbolos y letras) para cada archivo inkml\n",
    "\n",
    "with open(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TestINKML\\labels.json\", 'r') as f:\n",
    "    items = json.load(f)\n",
    "for key in items:\n",
    "    # se convierte a string\n",
    "    items[key] = ' '.join(map(str, items[key]))\n",
    "    # se anade al inicio y final los labels start y end para indicar inicio y final del ground truth\n",
    "    items[key] = '<start> '+items[key] + ' <end>'\n",
    "    # se vuelven a separar\n",
    "    items[key] = re.sub(r\"\\s+\", \" \", items[key])\n",
    "# se obtiene la cantidad de archvios inkml a tratar\n",
    "keys = list(items.keys())\n",
    "# se anade el groud truth de cada archivo a una lista\n",
    "data = [ items[key] for key in keys ]\n",
    "# se instancia la funcion tokenizer con los parametros establecidos en un principio\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token, filters='', lower=False)\n",
    "\n",
    "with open(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TrainINKML\\tokens.json\", 'r') as f:\n",
    "        word_index = json.load(f)\n",
    "tokenizer.word_index = word_index\n",
    "# se convierten los labels a tokens para cada archivo inkml\n",
    "# por ejemplo ['a','b','c'] se convierte a [1,2,3] asumiendo que estos son sus tokens\n",
    "test_sequences = tokenizer.texts_to_sequences(data)\n",
    "# se obtiene la longitud del label mas grande\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "# se asocian los tokens a cada archivo haciendo un pad hacia la maxima longitud\n",
    "# tal que todos los archivos tenga por label una lista de la misma longitud, rellenando con ceros\n",
    "# aquellos tokens que se encuentran en una longitud mayor al verdadero para determinado archivo\n",
    "test_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (150, 180, 1)\n",
      "Label:  [ 4 85 15 13 39 11  2 29 15  6  3  8  2 19  3 46 11 29 12 13 19 12  7 14\n",
      " 47 14 31  8  7  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "data= {}\n",
    "LABELS_PATH = r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TestINKML\\labelsPerFile.json\"\n",
    "with open(LABELS_PATH,  'r') as f:\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "files_img = {}\n",
    "# se obtienen los nombres de las imagenes\n",
    "with open(r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TestINKML\\files_img.json\", 'r') as f:\n",
    "    files_img = json.load(f)\n",
    "    f.close()\n",
    "keys = list(files_img.keys())\n",
    "files = list(data.keys())\n",
    "# se obtiene el dataset a partir del diccionario que contiene los tokens por archivo\n",
    "list_ds = tf.data.Dataset.list_files(files)\n",
    "\n",
    "# funcion que obtiene la imagen y su label, tiene como parametro el path de la imagen\n",
    "def getImagesTest(f):\n",
    "    index = keys.index(f)\n",
    "    # se lee la imagen\n",
    "    image = tf.io.read_file(f)\n",
    "    # se decodifica el formato jpg y se establece como imagen en un solo canal, es decir\n",
    "    # blanco y negro\n",
    "    image = tf.image.decode_jpeg(image, channels = 1)  \n",
    "    # se convierte su tipo de dato a flotante\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # se redimensiona para que concuerde con el modelo del encoder\n",
    "    image = tf.image.resize(image, [150, 180] ) \n",
    "    # se retorna la imagen (su matriz de pixels) y su label\n",
    "    return image, test_padded[index]\n",
    "\n",
    "# define el dataset a utilizar como un mapeo de los archvios dentro de list_ds que manda llamar a la funcion getImagesTest\n",
    "testDataset = list_ds.map(lambda x: tf.py_function(getImagesTest, [x], [tf.float32, tf.int32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# solo para verificar que funciona, imprime las dimensioens de la imagen y su label\n",
    "for image, label in testDataset.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "# se hace shuffle al conjunto de daots de prueba y se establece un tamano de batch \n",
    "# similar al de entrenamiento\n",
    "testDataset = testDataset.shuffle(buffer_size=20)\n",
    "testDataset = testDataset.batch(8)\n",
    "print(len(testDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "error",
     "timestamp": 1636324147325,
     "user": {
      "displayName": "ELIAN CRUZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYBTy2d5vbtWP2HeqOZoD_CuIJwgzN78T8neHL=s64",
      "userId": "03850060041876825577"
     },
     "user_tz": 480
    },
    "id": "vXO8FV4Km07D",
    "outputId": "4f410186-f0c2-4cec-a3fd-d4ddf75a6433"
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "def test(image, groundTruth):\n",
    "    global glob_hidden\n",
    "    loss = 0\n",
    "    acc = []\n",
    "    temp = []\n",
    "    # el estado del gru se inicializa como ceros\n",
    "    hidden = decoder.reset(groundTruth.shape[0])\n",
    "    # el input del decoder se inicializa como un tensor con valores para el primer token en todas las imagenes\n",
    "    input_decoder = tf.constant([word_index['<start>']] * groundTruth.shape[0])\n",
    "    count = 0\n",
    "    # se llama al encoder con la imagen la cual regresa el feature map con las caracteristicas extraidas de la imagen\n",
    "    feature_map = encoder(image)\n",
    "    # se itera por la cantidad de posibles palabras en un groundTruth\n",
    "    for i in range(1, groundTruth.shape[1]):\n",
    "        # para cada palabra se verifica si su tensor corresponde a solo valores en cero, para lo cual se rompe el ciclo\n",
    "        # terminando la prueba de este batch\n",
    "        sum_ = tf.reduce_sum(groundTruth[:,i])\n",
    "        allZero = tf.equal(sum_, 0)\n",
    "        if (allZero):\n",
    "            break\n",
    "        # si existe por lo menos un label por predecir entonces se llama al decoder envindole \n",
    "        # su respectivo input, las salidas del encoder y el hidden state anterior\n",
    "        pred, hidden = decoder(feature_map, input_decoder, hidden)\n",
    "        temp.append([groundTruth[:,i], K.argmax(pred, 1)])\n",
    "        # se calcula el error de las predicciones y se suma al actual para el batch\n",
    "        loss += loss_function(groundTruth[:, i], pred)\n",
    "        acc.append(accuracy(groundTruth[:, i], pred).numpy())\n",
    "        #print(i,\"loss:\", loss)\n",
    "        # la siguiente entrada para el decoder son las anotaciones para la iteracion actual\n",
    "        # es decir, la anterior para la siguiente interacion (h-1)\n",
    "        input_decoder = groundTruth[:, i]\n",
    "        count +=1\n",
    "    test_data.append(temp)\n",
    "    # dado que es la prueba, se elimina la necesidad de calcular gradientes y ajustar los pesos\n",
    "    # de los distintos layers del modelo\n",
    "    # una vez que termina la entrada del batch se calcula el error total con el error obtenido\n",
    "    total_loss = (loss / count)\n",
    "    # se regresa el error, el error total y la presicion\n",
    "    return loss, total_loss, sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "W8t83To6m07E",
    "outputId": "c383d80f-ce29-4809-c89d-7f2488277874"
   },
   "outputs": [],
   "source": [
    "def testData(dataset):\n",
    "    print(len(dataset))\n",
    "    # se itera por las epocas de entrenamiento\n",
    "    total_loss = 0\n",
    "    num = 0\n",
    "    # para cada batch de images en el dataset\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        # se llama a la funcion de entrenamiento para cada batch enviando el tensor de imagenes y de labels\n",
    "        # obteniendo los erroes\n",
    "        print(\"batch\",batch+1, end=\" \")\n",
    "        if (batch+1 == 1023):\n",
    "            break\n",
    "        batch_loss, t_loss, acc = test(img_tensor, target)\n",
    "        print(\"loss:\", t_loss,\"accuracy:\",acc , end=\"\\n\")\n",
    "        total_loss += t_loss\n",
    "        num +=1\n",
    "    print('===============>  train-loss=%.3f' % (total_loss/num))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "batch 1 loss: tf.Tensor(3.163328, shape=(), dtype=float32) accuracy: 0.6747023910284042\n",
      "batch 2 loss: tf.Tensor(3.2258883, shape=(), dtype=float32) accuracy: 0.5390350948038855\n",
      "batch 3 loss: tf.Tensor(2.468643, shape=(), dtype=float32) accuracy: 0.5084686184471304\n",
      "batch 4 loss: tf.Tensor(2.2772663, shape=(), dtype=float32) accuracy: 0.6149891804565083\n",
      "batch 5 loss: tf.Tensor(2.4477868, shape=(), dtype=float32) accuracy: 0.5775974077934568\n",
      "batch 6 loss: tf.Tensor(2.9205372, shape=(), dtype=float32) accuracy: 0.6448529476628584\n",
      "batch 7 loss: tf.Tensor(2.690727, shape=(), dtype=float32) accuracy: 0.6041805148124695\n",
      "batch 8 loss: tf.Tensor(2.885446, shape=(), dtype=float32) accuracy: 0.6088359819518195\n",
      "batch 9 loss: tf.Tensor(2.4384165, shape=(), dtype=float32) accuracy: 0.49202658236026764\n",
      "batch 10 loss: tf.Tensor(2.67287, shape=(), dtype=float32) accuracy: 0.6006084693802728\n",
      "batch 11 loss: tf.Tensor(2.9448988, shape=(), dtype=float32) accuracy: 0.5000000046359168\n",
      "batch 12 loss: tf.Tensor(2.77842, shape=(), dtype=float32) accuracy: 0.5383333371745216\n",
      "batch 13 loss: tf.Tensor(2.6597402, shape=(), dtype=float32) accuracy: 0.5692911286923018\n",
      "batch 14 loss: tf.Tensor(3.0004208, shape=(), dtype=float32) accuracy: 0.39573643928350405\n",
      "batch 15 loss: tf.Tensor(2.9189038, shape=(), dtype=float32) accuracy: 0.4702910125255585\n",
      "batch 16 loss: tf.Tensor(2.4634807, shape=(), dtype=float32) accuracy: 0.46215393550174183\n",
      "batch 17 loss: tf.Tensor(3.218082, shape=(), dtype=float32) accuracy: 0.5728696835668463\n",
      "batch 18 loss: tf.Tensor(3.1856477, shape=(), dtype=float32) accuracy: 0.5192691111980483\n",
      "batch 19 loss: tf.Tensor(2.9996338, shape=(), dtype=float32) accuracy: 0.5582539770338271\n",
      "batch 20 loss: tf.Tensor(2.8393888, shape=(), dtype=float32) accuracy: 0.5930875635916187\n",
      "batch 21 loss: tf.Tensor(2.7781365, shape=(), dtype=float32) accuracy: 0.49984127713574306\n",
      "batch 22 loss: tf.Tensor(2.8507473, shape=(), dtype=float32) accuracy: 0.45131579433616836\n",
      "batch 23 loss: tf.Tensor(2.9054797, shape=(), dtype=float32) accuracy: 0.6795518275569467\n",
      "batch 24 loss: tf.Tensor(2.8685615, shape=(), dtype=float32) accuracy: 0.5955532272072399\n",
      "batch 25 loss: tf.Tensor(3.185253, shape=(), dtype=float32) accuracy: 0.6374074127939012\n",
      "batch 26 loss: tf.Tensor(3.0241306, shape=(), dtype=float32) accuracy: 0.5514814843734105\n",
      "batch 27 loss: tf.Tensor(2.9924324, shape=(), dtype=float32) accuracy: 0.5479474612351122\n",
      "batch 28 loss: tf.Tensor(2.3845828, shape=(), dtype=float32) accuracy: 0.5948322527110577\n",
      "batch 29 loss: tf.Tensor(2.295958, shape=(), dtype=float32) accuracy: 0.5760052912765079\n",
      "batch 30 loss: tf.Tensor(2.4945567, shape=(), dtype=float32) accuracy: 0.574264711316894\n",
      "batch 31 loss: tf.Tensor(3.0849514, shape=(), dtype=float32) accuracy: 0.4869047654111211\n",
      "batch 32 loss: tf.Tensor(2.8183243, shape=(), dtype=float32) accuracy: 0.3941306780244029\n",
      "batch 33 loss: tf.Tensor(3.0020657, shape=(), dtype=float32) accuracy: 0.5936403556873924\n",
      "batch 34 loss: tf.Tensor(2.7273064, shape=(), dtype=float32) accuracy: 0.559191586666329\n",
      "batch 35 loss: tf.Tensor(2.7513638, shape=(), dtype=float32) accuracy: 0.5865646327535311\n",
      "batch 36 loss: tf.Tensor(2.8821552, shape=(), dtype=float32) accuracy: 0.619615803388032\n",
      "batch 37 loss: tf.Tensor(2.6536868, shape=(), dtype=float32) accuracy: 0.5640243960589897\n",
      "batch 38 loss: tf.Tensor(2.4364579, shape=(), dtype=float32) accuracy: 0.7196778733940685\n",
      "batch 39 loss: tf.Tensor(2.565953, shape=(), dtype=float32) accuracy: 0.4623280472225613\n",
      "batch 40 loss: tf.Tensor(3.1552193, shape=(), dtype=float32) accuracy: 0.5871212170882658\n",
      "batch 41 loss: tf.Tensor(2.7302916, shape=(), dtype=float32) accuracy: 0.5904208241507064\n",
      "batch 42 loss: tf.Tensor(2.3948176, shape=(), dtype=float32) accuracy: 0.47093023498390996\n",
      "batch 43 loss: tf.Tensor(3.0378485, shape=(), dtype=float32) accuracy: 0.411018832824951\n",
      "batch 44 loss: tf.Tensor(2.3830214, shape=(), dtype=float32) accuracy: 0.5518398318778385\n",
      "batch 45 loss: tf.Tensor(3.1999855, shape=(), dtype=float32) accuracy: 0.6187324962195229\n",
      "batch 46 loss: tf.Tensor(3.2529392, shape=(), dtype=float32) accuracy: 0.6314775952521492\n",
      "batch 47 loss: tf.Tensor(3.36619, shape=(), dtype=float32) accuracy: 0.5379902006072157\n",
      "batch 48 loss: tf.Tensor(2.8267097, shape=(), dtype=float32) accuracy: 0.5352116505304972\n",
      "batch 49 loss: tf.Tensor(1.8965225, shape=(), dtype=float32) accuracy: 0.5618466912973218\n",
      "batch 50 loss: tf.Tensor(3.1993103, shape=(), dtype=float32) accuracy: 0.5595805045394671\n",
      "batch 51 loss: tf.Tensor(2.266075, shape=(), dtype=float32) accuracy: 0.5176719609234068\n",
      "batch 52 loss: tf.Tensor(2.7837589, shape=(), dtype=float32) accuracy: 0.45284553635411146\n",
      "batch 53 loss: tf.Tensor(2.616974, shape=(), dtype=float32) accuracy: 0.663852819890687\n",
      "batch 54 loss: tf.Tensor(2.906901, shape=(), dtype=float32) accuracy: 0.4397361677240681\n",
      "batch 55 loss: tf.Tensor(2.6071072, shape=(), dtype=float32) accuracy: 0.5773255828508111\n",
      "batch 56 loss: tf.Tensor(3.1897619, shape=(), dtype=float32) accuracy: 0.599036288758119\n",
      "batch 57 loss: tf.Tensor(2.818129, shape=(), dtype=float32) accuracy: 0.7197368470461745\n",
      "batch 58 loss: tf.Tensor(2.8597615, shape=(), dtype=float32) accuracy: 0.42621816625428754\n",
      "batch 59 loss: tf.Tensor(2.973399, shape=(), dtype=float32) accuracy: 0.4445736498333687\n",
      "batch 60 loss: tf.Tensor(2.3090868, shape=(), dtype=float32) accuracy: 0.6013322000702223\n",
      "batch 61 loss: tf.Tensor(2.5685775, shape=(), dtype=float32) accuracy: 0.6083333379692502\n",
      "batch 62 loss: tf.Tensor(2.8733132, shape=(), dtype=float32) accuracy: 0.5091478730502882\n",
      "batch 63 loss: tf.Tensor(2.860965, shape=(), dtype=float32) accuracy: 0.6343254017687979\n",
      "batch 64 loss: tf.Tensor(2.8194282, shape=(), dtype=float32) accuracy: 0.4768549332091975\n",
      "batch 65 loss: tf.Tensor(2.679138, shape=(), dtype=float32) accuracy: 0.40465116535508355\n",
      "batch 66 loss: tf.Tensor(2.6776266, shape=(), dtype=float32) accuracy: 0.5691893522938093\n",
      "batch 67 loss: tf.Tensor(1.7078726, shape=(), dtype=float32) accuracy: 0.703917053438002\n",
      "batch 68 loss: tf.Tensor(2.6289408, shape=(), dtype=float32) accuracy: 0.6124074118004905\n",
      "batch 69 loss: tf.Tensor(2.5798056, shape=(), dtype=float32) accuracy: 0.5616931276188957\n",
      "batch 70 loss: tf.Tensor(2.816259, shape=(), dtype=float32) accuracy: 0.4916666679476437\n",
      "batch 71 loss: tf.Tensor(3.328311, shape=(), dtype=float32) accuracy: 0.4122093098108159\n",
      "batch 72 loss: tf.Tensor(2.921081, shape=(), dtype=float32) accuracy: 0.5220720723674104\n",
      "batch 73 loss: tf.Tensor(2.9916716, shape=(), dtype=float32) accuracy: 0.469433722463814\n",
      "batch 74 loss: tf.Tensor(2.778685, shape=(), dtype=float32) accuracy: 0.5620431948539822\n",
      "batch 75 loss: tf.Tensor(3.1249354, shape=(), dtype=float32) accuracy: 0.49019934444926505\n",
      "batch 76 loss: tf.Tensor(2.9042225, shape=(), dtype=float32) accuracy: 0.555733086639329\n",
      "batch 77 loss: tf.Tensor(2.4825773, shape=(), dtype=float32) accuracy: 0.6303571493110873\n",
      "batch 78 loss: tf.Tensor(2.3895087, shape=(), dtype=float32) accuracy: 0.5179924297739159\n",
      "batch 79 loss: tf.Tensor(3.3339624, shape=(), dtype=float32) accuracy: 0.42578463662754407\n",
      "batch 80 loss: tf.Tensor(3.1966574, shape=(), dtype=float32) accuracy: 0.4984773019718569\n",
      "batch 81 loss: tf.Tensor(2.4986463, shape=(), dtype=float32) accuracy: 0.6514724355779196\n",
      "batch 82 loss: tf.Tensor(3.0441887, shape=(), dtype=float32) accuracy: 0.5237514561996227\n",
      "batch 83 loss: tf.Tensor(2.9677002, shape=(), dtype=float32) accuracy: 0.4972329522306855\n",
      "batch 84 loss: tf.Tensor(2.380651, shape=(), dtype=float32) accuracy: 0.5611678060321581\n",
      "===============>  train-loss=2.788\n"
     ]
    }
   ],
   "source": [
    "testData(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 128)\n"
     ]
    }
   ],
   "source": [
    "print(glob_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6872888732218218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_labels = [[] for _ in range(671)]\n",
    "final_pred = [[] for _ in range(671)]\n",
    "index = 0\n",
    "for i in test_data:\n",
    "    for j in i:\n",
    "        for k, count in zip(j[0].numpy(), range(8)):\n",
    "            final_labels[index+count].append(k)\n",
    "        for k, count in zip(j[1].numpy(), range(8)):\n",
    "            final_pred[index+count].append(k)\n",
    "    index+=8\n",
    "        #final_labels[index].append(j[0].numpy())\n",
    "WER_ = 0\n",
    "for k in range(671):\n",
    "    a = ' '.join([str(i) for i in final_labels[k]])\n",
    "    b = ' '.join([str(i) for i in final_pred[k]])\n",
    "    WER_ += wer(a,b)\n",
    "print(WER_/671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 13, 31, 11, 21, 32, 13, 46, 12, 46, 11, 21, 14, 12, 31, 11, 6, 32, 13, 46, 12, 46, 11, 6, 14, 14, 15, 31, 11, 21, 31, 11, 6, 32, 13, 46, 11, 6, 12, 46, 11, 21, 14, 5]\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle r{\\left(r_{0} \\sin{\\left(\\theta - theta_{0} \\right)} - r_{1} \\sin{\\left(\\theta - theta_{1} \\right)} \\right)} = r_{0} r_{1} \\sin{\\left(- theta_{0} + theta_{1} \\right)}$"
      ],
      "text/plain": [
       "r(r_{0}⋅sin(θ - θ_{0}) - r_{1}⋅sin(θ - θ_{1})) = r_{0}⋅r_{1}⋅sin(-θ_{0} + θ_{1\n",
       "})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = np.random.randint(671)\n",
    "print(final_labels[k])\n",
    "original = parse_latex(' '.join([str(keys[i-1] ) for i in final_labels[k][:final_labels[k].index(5)]]))\n",
    "original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\frac = x \\cos { \\sin C x } \\theta _ 0 )\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([str(keys[i-1] ) for i in final_pred[k][:final_pred[k].index(5)]]))\n",
    "try:\n",
    "    prediction_lat = parse_latex(' '.join([str(keys[i-1] ) for i in final_pred[k][final_pred[k].index(5)]]))\n",
    "    prediction_lat\n",
    "except:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "zkWwM2kJm07F",
    "outputId": "aad06843-9658-4f3e-d10c-ce9e8e205cff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2 6 3 2 23 2 7 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB0AAAAwCAYAAADtoXHnAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACdklEQVRYCe2X31EbMRCHz5kUQEgHpgMgFcTpACYdQAcwebLfMkkHJCWEDkgJgQ5CCQwdmO87pIvO3Nk6HW+5ndlZWfvnp12tZN1svV5XQ2m1Wh3i8ws+Yvw41P9trgPB97D9CT/Ax/AcLqIhoGZ0KgoLuECYbRG9KfIa6TSBjizgdvepvNvrM1I7lXdXAd8Hg/1dhl362ZALn+vPS15awN7Fd/A9fIPuBzKLBoFmRcwwmhopo0jlJlN5y2uX4fn/lHe2XC6Hv0EzSthnws01m26kvuq8ynz2Y7sPjT3y0f0Z9gGeRaNBQfkGXwJ+mYWI0ahzGrKskP6vZtMoUFC+wGY6iIrLS3ZzkObI3ylimI+l9uvOrzzL31SjGNRAgRHPFACvkJ+SOStx6xxcL7CovDj7PjqOQSIAUoDz5HeFjYvzMzO+r4obyeBf0+BhvED+DYtK1Wa4x7xbMhw0BFwgr9OoYWzwe3Rm1kVWqCrZ096OBaz+Uu9Aq7/a0dfN1AJl8gSHR2SrIzeCnKA/2Jjr/YmtgJY1dvS/TFF6jZmFj+cj+AUFm6sXiu0TNtA1vt+jWd29YTW+0OXD8DvapPI8dU4VXWNsXaB73Cp7BL1D4ebHjjTjFqE/YyI7y2C/j2zObAxYg8YfAdhs3Tf3ISX3JOt7BV974wDZZGi8GLMFGhDiXdpsfAjivvQdhWZx2Ng4H5CNf1C6EK/EqvO5gsMNOg/6O4HgW8YfHSN7Cb3V0ber+z3bdde3jkwSzWwFPcPQbv6zCzD4Ciiw+79JzYXfmanWgJidAQQ9DeAMx1PXnsaodrLXli0v8KtRLyhA3q2WZLMhRoM/AdPMzz+0Mvp1AAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle \\frac{1}{\\sqrt{2}}$"
      ],
      "text/plain": [
       "1 \n",
       "──\n",
       "√2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sympy import *\n",
    "from sympy.parsing.latex import parse_latex\n",
    "import antlr4\n",
    "def evaluation(image):\n",
    "    hidden = decoder.reset(groundTruth.shape[0])\n",
    "\n",
    "\n",
    "image = r\"G:\\Documents\\CROHME\\CROHME\\CROHME2013_data\\TestINKML\\images\\103_em_3.jpg\"\n",
    "img_val = tf.expand_dims(getProcessedImages2(image),0)\n",
    "g2 = tf.random.get_global_generator()\n",
    "\n",
    "image2 = r\"F:\\OneDrive\\Imágenes\\22.png\"\n",
    "img_val2 = tf.expand_dims(getProcessedImages2(image2),0)\n",
    "input_decoder = tf.constant([4] * 1)\n",
    "preds = []\n",
    "hidden = tf.constant([[[0.0]*128]*3]*1)\n",
    "\n",
    "\n",
    "batch = img_val\n",
    "feature_map = encoder.predict(batch)\n",
    "#batch = tf.concat([img_val, img_val2], 1)\n",
    "'''pred, hidden = decoder(feature_map, input_decoder, hidden)\n",
    "hypotheses = tf.math.top_k(pred, 10)\n",
    "input_decoder = tf.argmax(pred, 1)\n",
    "print(hypotheses.indices.numpy())\n",
    "a = np.random.randint(127)\n",
    "b = np.random.randint(127)\n",
    "pred, hidden = decoder(feature_map,tf.constant([13, 4]) , hidden)\n",
    "hypotheses = tf.math.top_k(pred, 10)\n",
    "print(hypotheses.indices.numpy())\n",
    "'''\n",
    "init_printing()\n",
    "for i in range(90):\n",
    "    pred, hidden = decoder(feature_map, input_decoder, hidden)\n",
    "    input_decoder = K.argmax(pred, 1)\n",
    "    preds.append(input_decoder)\n",
    "    #print(input_decoder.numpy()[0], end=\" \" )\n",
    "    if (input_decoder.numpy()[0] == 5):\n",
    "        break\n",
    "keys = list(word_index.keys())\n",
    "render = ''\n",
    "for i in preds:\n",
    "    print(i.numpy()[0], end=\" \")\n",
    "    render += keys[i.numpy()[0]-1] \n",
    "render = parse_latex(str(render))\n",
    "render\n",
    "#[16  2 13 23 10 41 17 64  6 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProcessedImages2(f):\n",
    "    #index = keys.index(f)\n",
    "    # se lee la imagen\n",
    "    image = tf.io.read_file(f)\n",
    "    # se decodifica el formato jpg y se establece como imagen en un solo canal, es decir\n",
    "    # blanco y negro\n",
    "    image = tf.image.decode_jpeg(image, channels = 1, dct_method = 'INTEGER_ACCURATE')  \n",
    "    # se convierte su tipo de dato a flotante\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # se redimensiona para que concuerde con el modelo del encoder\n",
    "    image = tf.image.resize(image, [150, 180] ) \n",
    "    # se retorna la imagen (su matriz de pixels) y su label\n",
    "    return image#, train_padded[index] #[data[str(tf.constant(f).numpy())[2:-1].replace('\\\\\\\\', '\\\\') ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys[96] = \"\\\\lt\"\n",
    "keys[105] = \"\\\\gt\"\n",
    "keys[14]='='\n",
    "keys[97] = '\\\\!'\n",
    "keys[39] = '\\\\,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "_modelo_en_drive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
