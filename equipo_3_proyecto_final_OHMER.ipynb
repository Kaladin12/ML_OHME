{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2702,
     "status": "ok",
     "timestamp": 1636613605638,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "-66WsO14m06r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# dependencia externa cuyo código se adjunta en el mismo directorio en el que se encuentra este archivo\n",
    "import inkml2img\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Yo\\\\Desktop\\\\ML_OHME'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = os.path.abspath('')\n",
    "dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<<<<<<<<<<<<NO ES NECESARIO EJECUTAR>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "# En esta celda se convierten los archivos inkml a .jpg con ayuda del modulo inkml2img\n",
    "# PATH indica la ubicacion de los archivos .inkml\n",
    "PATH = dir+r\"\\\\CROHME\\\\TrainINKML\"\n",
    "# DIR_PATH indica el directorio donde se guardarán las imagenes\n",
    "DIR_PATH = dir+r\"\\\\CROHME\\\\TrainINKML\\\\images\"\n",
    "# se obtiene el nombre de las carpetas que contienen los archivos .inkml\n",
    "dataSets = os.listdir(PATH)\n",
    "# se genera una lista con listas vacias (la cantidad de carpetas en dataSets)\n",
    "arr = [[] for _ in range(len(dataSets))]\n",
    "print(dataSets)\n",
    "# se itera por cada carpeta de archivos .inkml\n",
    "for i in range(len(dataSets)):\n",
    "    # se agrega a la lista los archivos en determinada carpeta (la de la iteracion actual)\n",
    "    arr[i] = os.listdir(PATH+'\\\\'+dataSets[i])\n",
    "    # se itera por los archivos .inkml de la carpeta\n",
    "    for t in arr[i]:\n",
    "        # si la extension no es .lg\n",
    "        if ('.lg' not in t):\n",
    "            # se llama a la funcion inkml2img del modulo hominimo, especificando la ruta del inkml y la ruta a guardar la imagen\n",
    "            inkml2img.inkml2img(PATH+'\\\\'+dataSets[i]+'\\\\'+t,DIR_PATH+'\\\\'+t[:-5]+'jpg')\n",
    "            #print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvuvu5h_m063"
   },
   "outputs": [],
   "source": [
    "# <<<<<<<<<<<<<<NO ES NECESARIO EJECUTAR>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "\n",
    "# en esta celda se obtienen los labels (el ground truth en latex) asociado a cada archivo .inkml\n",
    "# Paths indicando la direccion donde guardar y leer los archivos del conjunto de datos\n",
    "DIR_PATH = dir+r\"\\\\CROHME\\\\TrainINKML\\\\\"\n",
    "LABELS_PATH = dir+r\"\\\\CROHME\\TrainINKML\\\\\"\n",
    "# diccionario donde el ground truth se guardara para cada archivo\n",
    "items = {}\n",
    "index = 0\n",
    "count = 0\n",
    "# se itera por la cantidad de carpetas que contienen archivos .inkml\n",
    "for t in arr:\n",
    "    # se itera por cada archivo inkml\n",
    "    for i in t:\n",
    "        if (\".lg\" not in i):\n",
    "            # se obtiene el path del archivo concatenando DIR_PATH con el nombre de la carpeta y del archivo\n",
    "            imgPath = DIR_PATH+\"\\\\\"+ dataSets[index]+ \"\\\\\"+i\n",
    "            # dado que .inkml es un tipo de XML, tiene una estructura de arbol, por lo que se utiliza \n",
    "            # la funcion ET.parse para hacer uso de esta estructura\n",
    "            tree = ET.parse(imgPath)\n",
    "            # se otbiene la raiz del archivo (primera etiqueta en el .inkml)\n",
    "            root = tree.getroot()\n",
    "            count =0\n",
    "            # se itera por cada hijo de la raiz\n",
    "            for item in root:\n",
    "                # si el hijo posee texto entonces es posible que contenga el ground truth (label) deseado\n",
    "                if item.text:\n",
    "                    # existen tres posibilidades debido a la forma en que fueron codificados los archivos en distintas carpetas\n",
    "                    # el primero es que el groud truth en latex se encuetre entre simbolos de dolar $\n",
    "                    if \"$\" in item.text:\n",
    "                        current = item.text\n",
    "                        # se añade como key del diccionario el nombre del archivo y se le asocia el ground truth en latex\n",
    "                        items[DIR_PATH+\"\\\\\"+ dataSets[index]+ \"\\\\\"+i] = current[1:-1]\n",
    "                        break\n",
    "                    # para la carpeta MathBrush cuyo formato de groundTruth es distinto\n",
    "                    if \"\\\\\" in item.text and dataSets[index] == \"MathBrush\":\n",
    "                        current = item.text\n",
    "                        items[DIR_PATH+\"\\\\\"+ dataSets[index]+ \"\\\\\"+i] = current[1:-1]\n",
    "                        break\n",
    "                    # para la carpeta KAIST el latex se encuentra sin elementos externos, por lo que se agrega sin mas\n",
    "                    if (count ==1 and dataSets[index]==\"KAIST\" and len(item.text)>2):\n",
    "                        current = item.text\n",
    "                        items[DIR_PATH+\"\\\\\"+ dataSets[index]+ \"\\\\\"+i] = current\n",
    "                        break\n",
    "                count +=1\n",
    "    index +=1 \n",
    "# se verifica que el comando log no se encuentre como una secuencia de letras separadas\n",
    "for key in items:\n",
    "    if ('l o g' in items[key]):\n",
    "        # si se encuentra se reemplaza por su respectivo comando en latex correcto\n",
    "        items[key] = items[key].replace('l o g', '\\\\log')\n",
    "# se guarda el diccionario en formato json\n",
    "with open(dir+r\"\\\\CROHME\\\\TrainINKML\\\\labels.json\", 'w') as f:\n",
    "    json.dump(items, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_6f1juom065"
   },
   "outputs": [],
   "source": [
    "# <<<<<<<<<<<<<<NO ES NECESARIO EJECUTAR>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "\n",
    "# se realiza un filtrado de los groud truth para separar cada comando, letra y numero presente en el dataset\n",
    "# establece los comandos a borrar, ya que no proporcionan informacion relevante para la ecuacion\n",
    "delete = ['\\\\Bigg','\\\\left','\\\\right','\\\\Big','\\\\mathrm']\n",
    "# comandos a reemplazar por el comando latex correcto\n",
    "replace = {'\\\\to':'\\\\rightarrow', '\\\\gt':'>', '\\\\lt':'<'}\n",
    "# simbolos a los que se les añade espacios en blanco antes y despues para un mejor tratamiento\n",
    "add = ['_','{','}','=','(',')','-','+','^','[',']', ',']\n",
    "count = 0\n",
    "# se iteran los archivos .inkml, items representa el mismo diccionario guardado en labels.json\n",
    "for key in items:\n",
    "    # si existe alguno de los comandos descritos en las listas y diccionario anteriores se realizan la soperaciones\n",
    "    # correspondientes\n",
    "    for dele in delete:\n",
    "        if dele in items[key]:\n",
    "            items[key] = items[key].replace(dele, \"\")\n",
    "    for rep in replace:\n",
    "        if (rep in items[key]):\n",
    "            items[key] = items[key].replace(rep, replace[rep])\n",
    "    for it in add:\n",
    "        if (it in items[key]):\n",
    "            items[key] = items[key].replace(it, \" \"+it+\" \")\n",
    "    # se agrega un espacio en blanco antes de \\\\ para que pueda diferenciarse a los comandos del resto de letras o numeros\n",
    "    if ('\\\\' in items[key]):\n",
    "        items[key] = items[key].replace('\\\\', \" \\\\\")\n",
    "    # se separa el ground truth, siendo la condicion para separa el que existan uno o mas espacios en blanco entre caracteres\n",
    "    items[key] = re.split(r'\\s+', items[key])\n",
    "    count = 0\n",
    "    # se itera sobre la lista generada de letras, numeros, simbolos y comandos para cada archivo inkml\n",
    "    # separando los caracteres que se encuentren juntos y no sean parte de un comando, \n",
    "    # por ejemplo 'abc' se separa en 'a', 'b', 'c'\n",
    "    for a in items[key]:\n",
    "        if ('\\\\' not in a and len(a)>1):\n",
    "            uno = items[key][:count]\n",
    "            dos = re.split('', a)[1:-1]\n",
    "            tres = items[key][count+1:]\n",
    "            count+= len(dos)-1\n",
    "            uno.extend(dos)\n",
    "            uno.extend(tres)\n",
    "            items[key] = uno\n",
    "        count+=1\n",
    "    if (items[key][-1]==\"\"):\n",
    "        items[key] = items[key][:-1]\n",
    "    if (items[key][0]==\"\"):\n",
    "        items[key] = items[key][1:]\n",
    "# se guarda en labels.json\n",
    "with open(dir+r\"\\\\CROHME\\\\TrainINKML\\\\labels.json\", 'w') as f:\n",
    "    json.dump(items, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4394,
     "status": "ok",
     "timestamp": 1636613612533,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "EeL-Ufr0m066",
    "outputId": "3d802666-c333-4f39-c9e0-f8223f78c794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> S = ( \\sum _ { i = 1 } ^ { n } \\theta _ i - ( n - 2 ) \\pi ) r ^ 2 <end>\n"
     ]
    }
   ],
   "source": [
    "# se define el numero maximo de palabras a tokenizar\n",
    "num_words = 1000\n",
    "# token para labels desconocidas\n",
    "oov_token = '<UNK>'\n",
    "# parametros de inicializacion del tokenizer de tensorflow\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "items = {}\n",
    "# se obtienen las listas de labels (numeros, comandos latex, simbolos y letras) para cada archivo inkml\n",
    "# derivados del preprocesamiento previo\n",
    "with open(dir+r\"\\\\CROHME\\\\TrainINKML\\\\labels.json\", 'r') as f:\n",
    "    items = json.load(f)\n",
    "tokens = {}\n",
    "count = 0\n",
    "# se itera para cada archivo\n",
    "for key in items:\n",
    "    # se convierte a string\n",
    "    items[key] = ' '.join(map(str, items[key]))\n",
    "    # se anade al inicio y final los labels start y end para indicar inicio y final del ground truth\n",
    "    items[key] = '<start> '+items[key] + ' <end>'\n",
    "    # se vuelven a separar\n",
    "    items[key] = re.sub(r\"\\s+\", \" \", items[key])\n",
    "# se obtiene la cantidad de archvios inkml a tratar\n",
    "keys = list(items.keys())\n",
    "# se anade el groud truth de cada archivo a una lista\n",
    "data = [ items[key] for key in keys ]\n",
    "print(data[0])\n",
    "# se instancia la funcion tokenizer con los parametros establecidos en un principio\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token, filters='', lower=False)\n",
    "# se obtienen los tokens para el conjunto de datos, es decir, a cada comando de latex, letra, numero o simbolo\n",
    "# utilizado se le asigna un numero entero en base a su frecuencia de aparicion\n",
    "tokenizer.fit_on_texts(data)\n",
    "# se define la cantidad de palabras o tokens\n",
    "word_index = tokenizer.word_index\n",
    "# se guardan los tokens en tokens.json\n",
    "with open(dir+r\"\\\\CROHME\\\\TrainINKML\\\\tokens.json\", 'r') as f:\n",
    "        word_index = json.load(f)\n",
    "tokenizer.word_index = word_index\n",
    "# se convierten los labels a tokens para cada archivo inkml\n",
    "# por ejemplo ['a','b','c'] se convierte a [1,2,3] asumiendo que estos son sus tokens\n",
    "train_sequences = tokenizer.texts_to_sequences(data)\n",
    "# se obtiene la longitud del label mas grande\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "# se asocian los tokens a cada archivo haciendo un pad hacia la maxima longitud\n",
    "# tal que todos los archivos tenga por label una lista de la misma longitud, rellenando con ceros\n",
    "# aquellos tokens que se encuentran en una longitud mayor al verdadero para determinado archivo\n",
    "train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<<<<<<<<<<<<<NO ES NECESARIO EJECUTAR>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "\n",
    "# se cargan los datos generados anteriormente: los tokens y etiquetas sin toquenizar de los\n",
    "# archivos del dataset\n",
    "LABELS_PATH = r\"/content/drive/My Drive/Colab Notebooks/TrainINKML\"\n",
    "tokens = {}\n",
    "files_imgs = {}\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/TrainINKML/tokens.json\", 'r') as f:\n",
    "    tokens = json.load(f)\n",
    "tokensPerFile = {}\n",
    "items = {}\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/TrainINKML/labels.json\", 'r') as f:\n",
    "    items = json.load(f)\n",
    "# se guardan los tokens por archivo inkml haciendo referencia ahora a la imagen \n",
    "# generada a partir del archvio inkml\n",
    "for key in items:\n",
    "    newKey = LABELS_PATH +'/images/'+ key[len(LABELS_PATH)+15:][:-6].replace('\\\\','/')+'.jpg'\n",
    "    tokensPerFile[newKey] = []\n",
    "    files_imgs[newKey] = LABELS_PATH + key[43:].replace('\\\\','/')\n",
    "    for command in items[key]:\n",
    "        tokensPerFile[newKey].append(tokens[command])\n",
    "# se guarda el conjunto de datos tokenizado en un formato json para su posterior uso\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/TrainINKML/labelsPerFile.json\", 'w') as f:\n",
    "    json.dump(tokensPerFile, f, indent=4)\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/TrainINKML/files_img.json\", 'w') as f:\n",
    "    json.dump(files_imgs, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6884,
     "status": "ok",
     "timestamp": 1636613627037,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "ALcPac7Rm067",
    "outputId": "c8424e1c-96bf-45d2-9365-0782d58dcbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 180, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 148, 178, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 148, 178, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 148, 178, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 146, 176, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 146, 176, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 146, 176, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 144, 174, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 144, 174, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 144, 174, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 142, 172, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 142, 172, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 142, 172, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 71, 86, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 69, 84, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 69, 84, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 69, 84, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 67, 82, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 67, 82, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 67, 82, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 65, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 65, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 65, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 63, 78, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 63, 78, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 63, 78, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 39, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 29, 37, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 29, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 29, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 27, 35, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 27, 35, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 27, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 25, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 25, 33, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 25, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 23, 31, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 23, 31, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 23, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 9, 13, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 9, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 9, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 7, 11, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 7, 11, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 7, 11, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 5, 9, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 5, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 3, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 3, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 3, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 3, 128)         0         \n",
      "=================================================================\n",
      "Total params: 826,272\n",
      "Trainable params: 823,968\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# se define la arquitectura del encoder en base a Zhang (2017)\n",
    "class FCN_encoder(tf.keras.Model):\n",
    "    def __init__(self, dropout_rate = 0.2):\n",
    "        super(FCN_encoder, self).__init__()\n",
    "        # super dentro del constructor permite que la clase herede y se convierta en un objeto de Keras\n",
    "        \n",
    "        # bloque de convolucion 1, 32 filtros\n",
    "        # cada capa convolucional es seguida por un batch normalization y una capa de activacion\n",
    "        # relu, el movimiento del kernel de convolucion se establece como 1 y\n",
    "        # la dimension del kernel se establece como 3x3\n",
    "        self.conv_1_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)\n",
    "        self.batch_1_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_1_1 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_1_2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)\n",
    "        self.batch_1_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_1_2 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_1_3 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)\n",
    "        self.batch_1_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_1_3 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_1_4 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)\n",
    "        self.batch_1_4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_1_4 = tf.keras.layers.Activation('relu')\n",
    "        \n",
    "        # maxpooling para reducir el tamaño\n",
    "        self.maxPool_1 = tf.keras.layers.MaxPooling2D()\n",
    "        \n",
    "        # bloque convolucional 2, 64 filtros\n",
    "        self.conv_2_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_2_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_2_1 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_2_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_2_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_2_2 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_2_3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_2_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_2_3 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_2_4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_2_4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_2_4 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        self.maxPool_2 = tf.keras.layers.MaxPooling2D()\n",
    "\n",
    "        self.conv_3_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_3_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_3_1 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_3_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_3_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_3_2 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_3_3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_3_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_3_3 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_3_4 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)\n",
    "        self.batch_3_4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_3_4 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        self.maxPool_3 = tf.keras.layers.MaxPooling2D()\n",
    "\n",
    "        self.conv_4_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1)\n",
    "        self.batch_4_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_4_1 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_4_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1)\n",
    "        self.batch_4_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_4_2 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_4_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1)\n",
    "        self.batch_4_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_4_3 = tf.keras.layers.Activation('relu')\n",
    "        self.conv_4_4 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1)\n",
    "        self.drop_4 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.batch_4_4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act_4_4 = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        self.maxPool_4 = tf.keras.layers.MaxPooling2D()\n",
    "    # funcion de llamada de la clase, dentro de la cual se establece la secuencia de layers que el inpput seguirá\n",
    "    # cada 'bloque' de codigo representa al conjunto de layers que comparten la misma cantidad de filtros en el\n",
    "    # kernel de las capas convolucionales\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv_1_1(inputs)\n",
    "        x = self.batch_1_1(x)\n",
    "        x = self.act_1_1(x)\n",
    "        x = self.conv_1_2(x)\n",
    "        x = self.batch_1_2(x)\n",
    "        x = self.act_1_2(x)\n",
    "        x = self.conv_1_3(x)\n",
    "        x = self.batch_1_3(x)\n",
    "        x = self.act_1_3(x)\n",
    "        x = self.conv_1_4(x)\n",
    "        x = self.batch_1_4(x)\n",
    "        x = self.act_1_4(x)\n",
    "        x = self.maxPool_1(x)\n",
    "        \n",
    "        x = self.conv_2_1(x)\n",
    "        x = self.batch_2_1(x)\n",
    "        x = self.act_2_1(x)\n",
    "        x = self.conv_2_2(x)\n",
    "        x = self.batch_2_2(x)\n",
    "        x = self.act_2_2(x)\n",
    "        x = self.conv_2_3(x)\n",
    "        x = self.batch_2_3(x)\n",
    "        x = self.act_2_3(x)\n",
    "        x = self.conv_2_4(x)\n",
    "        x = self.batch_2_4(x)\n",
    "        x = self.act_2_4(x)\n",
    "        x = self.maxPool_2(x)\n",
    "        \n",
    "        x = self.conv_3_1(x)\n",
    "        x = self.batch_3_1(x)\n",
    "        x = self.act_3_1(x)\n",
    "        x = self.conv_3_2(x)\n",
    "        x = self.batch_3_2(x)\n",
    "        x = self.act_3_2(x)\n",
    "        x = self.conv_3_3(x)\n",
    "        x = self.batch_3_3(x)\n",
    "        x = self.act_3_3(x)\n",
    "        x = self.conv_3_4(x)\n",
    "        x = self.batch_3_4(x)\n",
    "        x = self.act_3_4(x)\n",
    "        x = self.maxPool_3(x)\n",
    "        \n",
    "        x = self.conv_4_1(x)\n",
    "        x = self.batch_4_1(x)\n",
    "        x = self.act_4_1(x)\n",
    "        x = self.conv_4_2(x)\n",
    "        x = self.batch_4_2(x)\n",
    "        x = self.act_4_2(x)\n",
    "        x = self.conv_4_3(x)\n",
    "        x = self.batch_4_3(x)\n",
    "        x = self.act_4_3(x)\n",
    "        x = self.conv_4_4(x)\n",
    "        x = self.drop_4(x)\n",
    "        x = self.batch_4_4(x)\n",
    "        x = self.act_4_4(x)\n",
    "        x = self.maxPool_4(x)\n",
    "        # el encoder retorna el feature map generado par la ultima capa de max pooling\n",
    "        return x\n",
    "    # se instancia una clase que permite conocer el tamaño de los parametros dentro de la red.\n",
    "    def model(self):\n",
    "        input = tf.keras.layers.Input(shape=(150, 180, 1))\n",
    "        return tf.keras.Model(inputs = input, outputs = self.call(input) )\n",
    "print(FCN_encoder().model().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1636613631367,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "rBdEvEt6m068"
   },
   "outputs": [],
   "source": [
    "# el modelo de atencion es instanciado\n",
    "class Attender(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Attender, self).__init__()\n",
    "        # se instancia los distintos dense layers parametrizados en Zhang\n",
    "        \n",
    "        # representa la capa que se utiliza para aprender del hidden state anterior del decoder\n",
    "        self.W_1 = tf.keras.layers.Dense(128)\n",
    "        # capa que aprendera del feature map generado por el encoder\n",
    "        self.U_a = tf.keras.layers.Dense(128)\n",
    "        # la dimension de atencion es 128\n",
    "        self.V_a = tf.keras.layers.Dense(128)\n",
    "    \n",
    "    def call(self, a, h):\n",
    "        # se espanden dimensiones para el hidden state\n",
    "        h_t = tf.expand_dims(h, 1)\n",
    "        # se calcula el estado intermedio llamando a los dense layers y la funcion de tangente \n",
    "        # hiperbolica con la suma de los dos dense layers que reciben como parametro el hidden state\n",
    "        # y el vector de anotacion\n",
    "        e_ti = self.V_a( (tf.nn.tanh( self.W_1(h_t) + self.U_a(a))))\n",
    "        # se aplica la activacion softmax al resultado\n",
    "        a_ti = tf.nn.softmax(e_ti)\n",
    "        # se calcula el vector de contexto multiplicando los coeficientes a_ti por el vector de anotacion\n",
    "        context = a_ti * a\n",
    "        # se obtiene la suma del resutado de la multiplicacion anterior\n",
    "        context = (tf.reduce_sum(tf.reduce_sum(context, axis =1), axis=1))\n",
    "        # se regresa el vector de contexto y los coeficientes a_ti\n",
    "        return context     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1636613633565,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "3j_uVk59m069"
   },
   "outputs": [],
   "source": [
    "# se define el modelo decoder que utiliza el gru\n",
    "class GRU_decoder(tf.keras.Model):\n",
    "    def __init__(self, dimension, units, label_len):\n",
    "        super(GRU_decoder, self).__init__()\n",
    "        # las unidades del gru\n",
    "        self.units = units\n",
    "        # la capa de embedding, establecida en Zhang como E\n",
    "        self.embedding = tf.keras.layers.Embedding(label_len, dimension)\n",
    "        # la capa del gru, se especifica que se desea el regreso de las secuencias y el ultimo estado calculado\n",
    "        self.gru = tf.keras.layers.GRU( self.units, return_sequences=True, return_state = True, recurrent_initializer='glorot_uniform')\n",
    "        # dense layers para el calculo de las probabilidades para cada palabra en el diccionario de palabras\n",
    "        # es decir, los tokens\n",
    "        # dense layer utilizado para la salida del GRU\n",
    "        self.fc1 = tf.keras.layers.Dense(128)\n",
    "        # dense layer utilizado para aprender del vector de contexto generado\n",
    "        self.fc2 = tf.keras.layers.Dense(128)\n",
    "        # dense layer utilizado para aprender de la salida anterior del decoder\n",
    "        self.fc3 = tf.keras.layers.Dense(128)\n",
    "        # la ultima capa se define con una activacion softmax\n",
    "        self.fc4 = tf.keras.layers.Dense(128, activation='softmax')\n",
    "        # se instancia el modelo de atencion\n",
    "        self.attention = Attender()\n",
    "      \n",
    "    def call(self, a,x, h):\n",
    "        # la llamada recibe la salida del encoder, la entrada del decoder (su salida anterios) \n",
    "        # y el estado anterior del decoder\n",
    "        # se llama al modelo de atencion y se recibe el vector de contexto y los coeficientes de atencion\n",
    "        context_v = self.attention(a,h)\n",
    "        x = self.embedding(x)\n",
    "        # se anade el contexto a la entrada del decoder para utilizar el conocimeinto previo\n",
    "        t = tf.concat([context_v,x], axis =-1)\n",
    "        # se expanden las dimensiones para hacer compatible el tensor con la entrada del GRU\n",
    "        t = tf.expand_dims(t, 1)\n",
    "        # se alimenta al gru con dicha informacion y se obtiene una salida y su estado actual\n",
    "        out, state = self.gru(t)\n",
    "        # se reduce la dimension de la salida del gru para hacer posible su \n",
    "        out = tf.reduce_sum(out, axis=1)\n",
    "        \n",
    "        # es necesario calcular las probabilidades condicionales, por lo que se utiliza \n",
    "        # la metodologia descrita por zhang al sumar las salidas de las capas \n",
    "        # que usan la informacion de distintias secciones del modelo encoder-decoder\n",
    "        # como entrada de una capa final en la cual se realiza la clasficacion\n",
    "        a = self.fc1(out)\n",
    "        b = self.fc2(context_v)\n",
    "        c = self.fc3(x)\n",
    "        # se suman los resultados de cada capa densa\n",
    "        res = a+b+c\n",
    "        # se obtienen las probabilidades gracias al softmax del ultimo layer\n",
    "        res = self.fc4(res)\n",
    "        # se regresan las probabilidades, el estado y los coeficientes de atencion\n",
    "        # las probabilidades corresponden a un tensor que posee las siguientes dimensiones:\n",
    "        # (tamano_batch, cantidad de palabras) donde la cantidad de palabras es la misma\n",
    "        # que los tokens producidos durante el preprocesamiento.\n",
    "        return res, state\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    # funcion especial para inicializar la entrada del GRU por primera vez como un tensor\n",
    "    # de ceros, establecido como la forma mas eficiente de inicializar este t\n",
    "    def reset(self, batch):\n",
    "        return tf.zeros((batch,3, self.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definicion de funciones para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1636613659199,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "IUTMherym06-"
   },
   "outputs": [],
   "source": [
    "# instanciacion del encoder\n",
    "encoder = FCN_encoder()\n",
    "# isntanciacion del decoder, con valores para la dimension del embedding, del gru y la cantidad de tokens\n",
    "decoder = GRU_decoder(128,128,len(list(word_index.keys()))+1)\n",
    "# se define una funcion de optimizacion para el modelo, el cual permite realizar la optimizacion en base\n",
    "# al gradiente de los errores\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# definimos el tipo de calculo para los errores o perdiada entre los correspondientes labels del ground \n",
    "#truth de cada imagen y las predicciones realizadas por el modelo\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define una fucnion de loss (perdida o error) que recibe como parametro un label del ground truth por imagen \n",
    "# en el batch las matrices de prediccion asociadas a cada una de esas imagenes\n",
    "def loss_function(real, pred):\n",
    "    # se realiza una conversion del tipo de dato para las probabilidades de las predicciones\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    # se establece una mascara para aquellas imagenes cuyo label sea 0, es decir, que\n",
    "    # es el padding agregado y que de esta forma no se considere su error para el error general\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # se calcula el error\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # se anade la mascara al los obtenido\n",
    "    loss_ *= mask\n",
    "    # se retorna la media de las perdidas para las imagenes en el batch\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define una funcion para el calculo de presicion debido a que el padding en el groundTruth de las\n",
    "# imagenes implica que los ceros utilizados para ello seran tomados en cuenta durante su calculo\n",
    "# con la funcion propia de tensorflow\n",
    "def accuracy(groundTruth, pred):\n",
    "    # recibe los labels correctos del batch y las predicciones para cada una de las clases (127)\n",
    "    # se obtiene el argmax o el mayor valor de cada prediccion\n",
    "    pred_values = K.cast(K.argmax(pred, 1), dtype='int32')\n",
    "    # se establece cuales de dichas predicciones son verdaderas con base al groundTruth o etiqueta\n",
    "    correct = K.cast(K.equal(groundTruth, pred_values), dtype='float32')\n",
    "    # se utiliza una mascara para establecer aquellos espacios dentro de la etiqueta que representan un\n",
    "    # cero utilizado para el padding\n",
    "    mask = K.cast(K.greater(groundTruth, 0), dtype='float32')\n",
    "    # se toman en cuenta solamente aquellos valores que no sean cero\n",
    "    n_correct = K.sum(mask * correct)\n",
    "    # se calcula la cantidad de etiquetas del groundTruth que no son padding\n",
    "    n_total = K.sum(mask)\n",
    "      # se retorna el ratio representando la fraccion de predicciones correctas para esta seccion del batch\n",
    "    return n_correct / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 2840,
     "status": "ok",
     "timestamp": 1636613665443,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "4sfEL4Psm06_"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "# se obtienen los tokens por imagen\n",
    "LABELS_PATH = dir+\"\\\\CROHME\\\\TrainINKML\\\\labelsPerFile.json\"\n",
    "with open(LABELS_PATH,  'r') as f:\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26336,
     "status": "ok",
     "timestamp": 1636613693505,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "qz69MKV0m07A",
    "outputId": "e23cfbfb-f6b7-4cff-d5cf-09646d3bd7a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (150, 180, 1)\n",
      "Label:  [ 4  2 20  2 49  3  9 13 91 14  3  5  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "files_img = {}\n",
    "# se obtienen los nombres de las imagenes\n",
    "with open(dir+\"\\\\CROHME\\\\TrainINKML\\\\files_img.json\", 'r') as f:\n",
    "    files_img = json.load(f)\n",
    "    f.close()\n",
    "for i in list(files_img.keys()):\n",
    "    files_img[dir+'\\\\CROHME\\\\TrainINKML'+i] = files_img.pop(i)\n",
    "keys = list(files_img.keys())\n",
    "for i in list(data.keys()):\n",
    "    data[dir+'\\\\CROHME\\\\TrainINKML'+i] = data.pop(i)\n",
    "files = list(data.keys())\n",
    "# se obtiene el dataset a partir del diccionario que contiene los tokens por archivo\n",
    "list_ds = tf.data.Dataset.list_files(files)\n",
    "\n",
    "# funcion que obtiene la imagen y su label, tiene como parametro el path de la imagen\n",
    "def getImages(f):\n",
    "    # se obtiene el indice del archivo para obtener sus labels (tokens)\n",
    "    index = keys.index(f)\n",
    "    # se lee la imagen desde su fuente\n",
    "    image = tf.io.read_file(f)\n",
    "    # se decodifica el formato jpg y se establece como imagen en un solo canal, es decir\n",
    "    # blanco y negro\n",
    "    image = tf.image.decode_jpeg(image, channels = 1)  \n",
    "    # se convierte su tipo de dato a flotante\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # se redimensiona para que concuerde con el input del modelo (encoder)\n",
    "    image = tf.image.resize(image, [150, 180] ) \n",
    "    # se retorna la imagen (su matriz de pixels) y su label\n",
    "    return image, train_padded[index]\n",
    "\n",
    "# instancia el dataset a utilizar como un mapeo de los archvios dentro de list_ds que manda llamar a la funcion getProcessedImages\n",
    "labeledDataset = list_ds.map(lambda x: tf.py_function(getImages, [x], [tf.float32, tf.int32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# solo para verificar que funciona, imprime las dimensioens de una imagen del dataset y su label\n",
    "for image, label in labeledDataset.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1636613699338,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "lKAOv3vQm07B",
    "outputId": "8ecbe86f-2946-4942-c923-b905a17032ae"
   },
   "outputs": [],
   "source": [
    "# se utiliza un shuffle para revolver las imagenes dentro del dataset y evitar\n",
    "# que imagenes continuas siempre sean tomadas dentro del mismo batch\n",
    "labeledDataset = labeledDataset.shuffle(buffer_size=20)\n",
    "# se establece el tamano de batch como 8, es decir, se ingresara al entrenamiento paquetes de 8 imagenes\n",
    "labeledDataset = labeledDataset.batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1636613701200,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "zqvyQ5XXm07C"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# se define la funcion que entrena al modelo de encoder-decoder\n",
    "# recibe como parametros el batch de imagenes y su ground truth (labels)\n",
    "def train(image, groundTruth):\n",
    "    loss = 0\n",
    "    acc = []\n",
    "    # el estado del gru se inicializa como ceros\n",
    "    hidden = decoder.reset(groundTruth.shape[0])\n",
    "    # el input del decoder se inicializa como un tensor con valores para el primer token en todas las imagenes\n",
    "    input_decoder = tf.constant([word_index['<start>']] * groundTruth.shape[0])\n",
    "    g1 = tf.random.Generator.from_seed(1)\n",
    "    #input_decoder = tf.expand_dims([[word_index['<start>']]*3] * groundTruth.shape[0], 1)\n",
    "    #print(input_decoder.shape, hidden.shape)\n",
    "    # ciclo que permite el entrenamiento al generar un entorno donde las variables de entrenamiento son 'vigiladas'\n",
    "    # durante el entrenamiento para poder corregirlas y ajusstar el modelo\n",
    "    count = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # se llama al encoder con la imagen la cual regresa el feature map con las caracteristicas extraidas de la imagen\n",
    "        feature_map = encoder(image)\n",
    "        # se itera por la cantidad de posibles palabras en un groundTruth\n",
    "        for i in range(1, groundTruth.shape[1]):\n",
    "            # para cada palabra se verifica si su tensor corresponde a solo valores en cero, para lo cual se rompe el ciclo\n",
    "            # terminando el entrenamiento de este batch\n",
    "            sum_ = tf.reduce_sum(groundTruth[:,i])\n",
    "            allZero = tf.equal(sum_, 0)\n",
    "            if (allZero):\n",
    "                break\n",
    "            # si existe por lo menos un label por predecir entonces se llama al decoder envindole \n",
    "            # su respectivo input, las salidas del encoder y el hidden state anterior\n",
    "            pred, hidden = decoder(feature_map,input_decoder, hidden)\n",
    "            # se calcula el error de las predicciones y se suma al actual para el batch\n",
    "            loss += loss_function(groundTruth[:, i], pred)\n",
    "            acc.append(accuracy(groundTruth[:, i], pred).numpy())\n",
    "            #print(i,\"loss:\", loss)\n",
    "            # la siguiente entrada para el decoder son las anotaciones para la iteracion actual\n",
    "            # es decir, la anterior para la siguiente interacion (h-1)\n",
    "            input_decoder = groundTruth[:, i]\n",
    "            count +=1\n",
    "    # se obtienen las variables a las que se les puede modificar los parametros para ajustarlas (entrenarlas)\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # se obtiene el gradiente de error en base al error obtenido\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    # se realiza el ajuste en base al gradiente de error\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    # una vez que termina la entrada del batch se calcula el error total con el error obtenido\n",
    "    total_loss = (loss / count)\n",
    "    # se regresa el error dividido entre el numero de iteraciones para el batch,\n",
    "    # el error total del batch (loss), asi como la media de la presicion en las predicciones del batch\n",
    "    return loss, total_loss, sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1636613704474,
     "user": {
      "displayName": "Kaladin Bridgeburner",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05763820716362505426"
     },
     "user_tz": 480
    },
    "id": "VT-y7pKJm07C"
   },
   "outputs": [],
   "source": [
    "# funcion que itera sobre el dataset para entrenarlo\n",
    "accuracies = []\n",
    "losses = []\n",
    "def trainData(dataset, loadEpoch, batchToLoad, load, EPOCHS = 2):\n",
    "    print(len(dataset))\n",
    "    # se itera por las epocas de entrenamiento\n",
    "    for epoch in range(EPOCHS):\n",
    "        # se inicializa la perdida como 0 para cada epoca de entrenamiento\n",
    "        total_loss = 0\n",
    "        num = 0\n",
    "        # se hace iterable el dataset para obtener cada batch de images de forma individual\n",
    "        # obteniendo el numero de batch, el tensor de imagenes (pixeles) y las etiquetas de cada imagen del batch\n",
    "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "            # se llama a la funcion de entrenamiento para cada batch enviando el tensor de imagenes y de labels\n",
    "            # obteniendo los erroes\n",
    "            print(\"batch\",batch+1, end=\" \")\n",
    "            # dado que el ultimo batch tiene longitud distinta, se omite en cada epoca\n",
    "            if (batch+1 == 1023):\n",
    "                break\n",
    "            # se llama a la funcion de entrenamiento, enviando como parametro el batch de imagenes y labels\n",
    "            batch_loss, t_loss, acc = train(img_tensor, target)\n",
    "            accuracies.append(acc)\n",
    "            losses.append(t_loss)\n",
    "            if (load):\n",
    "                break\n",
    "            # utilizado para guardar los pesos del encoder y decoder cada 100 batches\n",
    "            if ((batch+1)%100 == 0 ):\n",
    "                encoder.save_weights(dir+r\"\\CROHME\\TrainINKML\\train\\enc\\{0}_{1}.h5\".format(loadEpoch+1+epoch, batch),\n",
    "                    save_format='h5')\n",
    "                decoder.save_weights(dir+r\"\\CROHME\\TrainINKML\\train\\dec\\{0}_{1}.h5\".format(loadEpoch+1+epoch, batch),\n",
    "                    save_format='h5')\n",
    "            print(\"loss:\", t_loss,\"accuracy:\",acc , end=\"\\n\")\n",
    "            # se suma la perdida obtenida en cada batch\n",
    "            total_loss += t_loss\n",
    "            num +=1\n",
    "        if not load:    \n",
    "            print('Epoch {0:d}/{1:d}'.format(epoch+1, EPOCHS))\n",
    "            print('===============>  train-loss=%.3f' % (total_loss/num))\n",
    "        else:\n",
    "            break\n",
    "    print(\"accuracy:\",sum(accuracies)/len(accuracies))\n",
    "    print(\"loss:\", sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1023\n",
      "batch 1 accuracy: 0.013657407628165351\n",
      "loss: tf.Tensor(2.003983, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# se llama la funcion principal de entrenamiento para inicializar\n",
    "trainData(labeledDataset, 0, 0, True, 1)\n",
    "# se cargan los pesos de un entrenamiento anterior\n",
    "encoder.load_weights(dir+r\"\\enc.h5\")\n",
    "decoder.load_weights(dir+r\"\\dec.h5\")\n",
    "# se llama a la funcion de entrenamiento para entrenar haciendo uso de los pesos\n",
    "# obtenidos de los entrenamientos previos, continuando el aprendizaje\n",
    "#trainData(labeledDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5671823076589171\n",
      "loss: tf.Tensor(4.6591296, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\",sum(accuracies)/len(accuracies))\n",
    "print(\"loss:\", sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = {}\n",
    "# se obtienen las listas de labels (numeros, comandos latex, simbolos y letras) para cada archivo inkml\n",
    "import re\n",
    "with open(dir+r\"\\CROHME\\TestINKML\\labels.json\", 'r') as f:\n",
    "    items = json.load(f)\n",
    "for key in items:\n",
    "    # se convierte a string\n",
    "    items[key] = ' '.join(map(str, items[key]))\n",
    "    # se anade al inicio y final los labels start y end para indicar inicio y final del ground truth\n",
    "    items[key] = '<start> '+items[key] + ' <end>'\n",
    "    # se vuelven a separar\n",
    "    items[key] = re.sub(r\"\\s+\", \" \", items[key])\n",
    "# se obtiene la cantidad de archvios inkml a tratar\n",
    "keys = list(items.keys())\n",
    "# se anade el groud truth de cada archivo a una lista\n",
    "data = [ items[key] for key in keys ]\n",
    "# se instancia la funcion tokenizer con los parametros establecidos en un principio\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token, filters='', lower=False)\n",
    "\n",
    "with open(dir+r\"\\CROHME\\TrainINKML\\tokens.json\", 'r') as f:\n",
    "        word_index = json.load(f)\n",
    "tokenizer.word_index = word_index\n",
    "# se convierten los labels a tokens para cada archivo inkml\n",
    "# por ejemplo ['a','b','c'] se convierte a [1,2,3] asumiendo que estos son sus tokens\n",
    "test_sequences = tokenizer.texts_to_sequences(data)\n",
    "# se obtiene la longitud del label mas grande\n",
    "maxlen = max([len(x) for x in test_sequences])\n",
    "# se asocian los tokens a cada archivo haciendo un pad hacia la maxima longitud\n",
    "# tal que todos los archivos tenga por label una lista de la misma longitud, rellenando con ceros\n",
    "# aquellos tokens que se encuentran en una longitud mayor al verdadero para determinado archivo\n",
    "test_padded = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (150, 180, 1)\n",
      "Label:  [ 4 16  2 27  8  2 18  3 12  7 27  9  6  3  2 27  8  2 28  3  9 18 27 12\n",
      "  6  3  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "data= {}\n",
    "LABELS_PATH = dir+r\"\\CROHME\\TestINKML\\labelsPerFile.json\"\n",
    "with open(LABELS_PATH,  'r') as f:\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "files_img = {}\n",
    "# se obtienen los nombres de las imagenes\n",
    "with open(dir+r\"\\CROHME\\TestINKML\\files_img.json\", 'r') as f:\n",
    "    files_img = json.load(f)\n",
    "    f.close()\n",
    "for i in list(files_img.keys()):\n",
    "    files_img[dir+'\\\\CROHME\\\\TestINKML'+i] = files_img.pop(i)\n",
    "keys = list(files_img.keys())\n",
    "for i in list(data.keys()):\n",
    "    data[dir+'\\\\CROHME\\\\TestINKML'+i] = data.pop(i)\n",
    "files = list(data.keys())\n",
    "# se obtiene el dataset a partir del diccionario que contiene los tokens por archivo\n",
    "list_ds = tf.data.Dataset.list_files(files)\n",
    "\n",
    "# funcion que obtiene la imagen y su label, tiene como parametro el path de la imagen\n",
    "def getImagesTest(f):\n",
    "    index = keys.index(f)\n",
    "    # se lee la imagen\n",
    "    image = tf.io.read_file(f)\n",
    "    # se decodifica el formato jpg y se establece como imagen en un solo canal, es decir\n",
    "    # blanco y negro\n",
    "    image = tf.image.decode_jpeg(image, channels = 1)  \n",
    "    # se convierte su tipo de dato a flotante\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # se redimensiona para que concuerde con el modelo del encoder\n",
    "    image = tf.image.resize(image, [150, 180] ) \n",
    "    # se retorna la imagen (su matriz de pixels) y su label\n",
    "    return image, test_padded[index]\n",
    "\n",
    "# define el dataset a utilizar como un mapeo de los archvios dentro de list_ds que manda llamar a la funcion getImagesTest\n",
    "testDataset = list_ds.map(lambda x: tf.py_function(getImagesTest, [x], [tf.float32, tf.int32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# solo para verificar que funciona, imprime las dimensioens de la imagen y su label\n",
    "for image, label in testDataset.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "# se hace shuffle al conjunto de daots de prueba y se establece un tamano de batch \n",
    "# similar al de entrenamiento\n",
    "testDataset = testDataset.shuffle(buffer_size=20)\n",
    "testDataset = testDataset.batch(8)\n",
    "print(len(testDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "error",
     "timestamp": 1636324147325,
     "user": {
      "displayName": "ELIAN CRUZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYBTy2d5vbtWP2HeqOZoD_CuIJwgzN78T8neHL=s64",
      "userId": "03850060041876825577"
     },
     "user_tz": 480
    },
    "id": "vXO8FV4Km07D",
    "outputId": "4f410186-f0c2-4cec-a3fd-d4ddf75a6433"
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "accuracies = []\n",
    "losses = []\n",
    "def test(image, groundTruth):\n",
    "    global glob_hidden\n",
    "    loss = 0\n",
    "    acc = []\n",
    "    temp = []\n",
    "    # el estado del gru se inicializa como ceros\n",
    "    hidden = decoder.reset(groundTruth.shape[0])\n",
    "    # el input del decoder se inicializa como un tensor con valores para el primer token en todas las imagenes\n",
    "    input_decoder = tf.constant([word_index['<start>']] * groundTruth.shape[0])\n",
    "    count = 0\n",
    "    # se llama al encoder con la imagen la cual regresa el feature map con las caracteristicas extraidas de la imagen\n",
    "    feature_map = encoder(image)\n",
    "    # se itera por la cantidad de posibles palabras en un groundTruth\n",
    "    for i in range(1, groundTruth.shape[1]):\n",
    "        # para cada palabra se verifica si su tensor corresponde a solo valores en cero, para lo cual se rompe el ciclo\n",
    "        # terminando la prueba de este batch\n",
    "        sum_ = tf.reduce_sum(groundTruth[:,i])\n",
    "        allZero = tf.equal(sum_, 0)\n",
    "        if (allZero):\n",
    "            break\n",
    "        # si existe por lo menos un label por predecir entonces se llama al decoder envindole \n",
    "        # su respectivo input, las salidas del encoder y el hidden state anterior\n",
    "        pred, hidden = decoder(feature_map, input_decoder, hidden)\n",
    "        temp.append([groundTruth[:,i], K.argmax(pred, 1)])\n",
    "        # se calcula el error de las predicciones y se suma al actual para el batch\n",
    "        loss += loss_function(groundTruth[:, i], pred)\n",
    "        acc.append(accuracy(groundTruth[:, i], pred).numpy())\n",
    "        \n",
    "        #print(i,\"loss:\", loss)\n",
    "        # la siguiente entrada para el decoder son las anotaciones para la iteracion actual\n",
    "        # es decir, la anterior para la siguiente interacion (h-1)\n",
    "        input_decoder = tf.argmax(pred,1)\n",
    "        count +=1\n",
    "    test_data.append(temp)\n",
    "    # dado que es la prueba, se elimina la necesidad de calcular gradientes y ajustar los pesos\n",
    "    # de los distintos layers del modelo\n",
    "    # una vez que termina la entrada del batch se calcula el error total con el error obtenido\n",
    "    total_loss = (loss / count)\n",
    "    # se regresa el error, el error total y la presicion\n",
    "    return loss, total_loss, sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "W8t83To6m07E",
    "outputId": "c383d80f-ce29-4809-c89d-7f2488277874"
   },
   "outputs": [],
   "source": [
    "def testData(dataset):\n",
    "    print(len(dataset))\n",
    "    # se itera por las epocas de entrenamiento\n",
    "    total_loss = 0\n",
    "    num = 0\n",
    "    # para cada batch de images en el dataset\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        # se llama a la funcion de entrenamiento para cada batch enviando el tensor de imagenes y de labels\n",
    "        # obteniendo los erroes\n",
    "        print(\"batch\",batch+1, end=\" \")\n",
    "        if (batch+1 == 1023):\n",
    "            break\n",
    "        # se llama la funcion de prueba para obtener las predicciones y métricas relacionadas\n",
    "        batch_loss, t_loss, acc = test(img_tensor, target)\n",
    "        # se añade el accuracy y perdida a una lista en orden de obtener sus medias de prueba\n",
    "        accuracies.append(acc)\n",
    "        losses.append(t_loss)\n",
    "        print(\"loss:\", t_loss,\"accuracy:\",acc , end=\"\\n\")\n",
    "        total_loss += t_loss\n",
    "        num +=1\n",
    "    print('===============>  train-loss=%.3f' % (total_loss/num))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.07122624531210295\n",
      "loss: tf.Tensor(6.473276, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# se muestran las medias de las mettricas geenradas\n",
    "print(\"accuracy:\",sum(accuracies)/len(accuracies))\n",
    "print(\"loss:\", sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9243961303803582\n"
     ]
    }
   ],
   "source": [
    "# calculo de la metrica WER\n",
    "\n",
    "# se generan listas del tamano del conjunto de prueba para las etiquetas y predicciones\n",
    "final_labels = [[] for _ in range(671)]\n",
    "final_pred = [[] for _ in range(671)]\n",
    "index = 0\n",
    "# se itera sobre las predicciones generadas durante la prueba\n",
    "for i in test_data:\n",
    "    # dado que cada batch contiene informacion de 8 imagenes distintas, se itera sobre este\n",
    "    for j in i:\n",
    "        # se anaden las predicciones y etiquetas originales a sus respectivas listas\n",
    "        for k, count in zip(j[0].numpy(), range(8)):\n",
    "            final_labels[index+count].append(k)\n",
    "        for k, count in zip(j[1].numpy(), range(8)):\n",
    "            final_pred[index+count].append(k)\n",
    "    index+=8\n",
    "# se instancia la variable que guardará el valor de la métrica\n",
    "WER_ = 0\n",
    "# se itera durante el tamaño del conjunto de datos de prueba\n",
    "# para obtener las predicciones en formato de texto\n",
    "for k in range(671):\n",
    "    # se genera una cadena de texto con las etiquetas correctas para la imagen\n",
    "    a = ' '.join([str(i) for i in final_labels[k]])\n",
    "    # se genera una cadena de caracteres con las predicciones estabelcidas por el modelo\n",
    "    b = ' '.join([str(i) for i in final_pred[k]])\n",
    "    # se utiliza la funcion wer del modulo jiwer, la cual requiere de entrada la cadena correcta y la predicha para\n",
    "    # calcular la métrica, de esta forma se asbtrae el calculo de forma manual\n",
    "    WER_ += wer(a,b)\n",
    "# se imprime la media de la métrica al dividir la suma sobre ela cantidad de imagenes de prueba\n",
    "print(WER_/671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permite obtener solamente la matriz de ppixeles de una imagen para poder realizar \n",
    "# predicciones con la misma\n",
    "def getPredImage(f):\n",
    "    # se lee la imagen\n",
    "    image = tf.io.read_file(f)\n",
    "    # se decodifica el formato jpg y se establece como imagen en un solo canal, es decir\n",
    "    # blanco y negro\n",
    "    image = tf.image.decode_jpeg(image, channels = 1, dct_method = 'INTEGER_ACCURATE')  \n",
    "    # se convierte su tipo de dato a flotante\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # se redimensiona para que concuerde con el modelo del encoder\n",
    "    image = tf.image.resize(image, [150, 180] ) \n",
    "    # se retorna la imagen (su matriz de pixels)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se importan las dependencias requeridas para la renderizacion de LaTeX de originales y predicciones\n",
    "from sympy import *\n",
    "from sympy.parsing.latex import parse_latex\n",
    "import antlr4\n",
    "# se establece el inicio de una impresion de LaTeX\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 11, 63, 15, 55, 9, 30, 53, 80, 9, 16, 2, 6, 3, 2, 18, 3, 13, 17, 12, 24, 14, 54, 5]\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle p_{A} = c \\left(L + \\frac{a - b}{3}\\right) + u$"
      ],
      "text/plain": [
       "          ⎛    a - b⎞    \n",
       "p_{A} = c⋅⎜L + ─────⎟ + u\n",
       "          ⎝      3  ⎠    "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imprime la etiqueta correcta\n",
    "# se obtiene una muestra aleatoria del conjunto de prueba\n",
    "k = np.random.randint(671)\n",
    "# se impirmen sus etiquetas\n",
    "print(final_labels[k])\n",
    "# se obtiene su representacion en latex al mapear los tokens a su respectivo comando de latex, numero o letra\n",
    "original = parse_latex(''.join([str(list(word_index.keys())[i-1] ) for i in final_labels[k][:final_labels[k].index(5)]]))\n",
    "# se renderiza la expresión\n",
    "original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\frac { 1 } { a }\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n"
     ]
    }
   ],
   "source": [
    "# imrpime la prediccion generada en caso de poder ser renderizada, so no imprime los comandos de latex como texto.\n",
    "print(' '.join([str(list(word_index.keys())[i-1]  ) for i in final_pred[k][:final_pred[k].index(5)]]))\n",
    "# dado que es posible que la predicción no sea renderizable se intenta hacerlo\n",
    "try:\n",
    "    # si es posible se renderiza\n",
    "    prediction_lat = parse_latex(' '.join([str(list(word_index.keys())[i-1] ) for i in final_pred[k][1:final_pred[k].index(5)]]))\n",
    "    prediction_lat\n",
    "except:\n",
    "    # en caso contrario se establece el error\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zkWwM2kJm07F",
    "outputId": "aad06843-9658-4f3e-d10c-ce9e8e205cff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\frac { 1 } { 2 } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n",
      "16 2 6 3 2 7 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAsAAAArCAYAAACuAHIQAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABVUlEQVQ4Ee2VzVHDQAxGMUMBmdCB0wE/HSQdQAvQATnaV+ggUAJ0YGqgg6QEhg7Me461bDCGWw6AZjTSSt9qZa1WLtq2PciprusT1o/wKfpb7jtygXGCeIBf4TO4hAcUYCNc6mXjDcLoAzocWL4x/IPz4uy3Gsf90dM8BfUiGolrtnmkOWyvvMAbuMF3j/wAu/iJ9vuBo9n8+jSKqqp2Z8FoLf7ADXbjywLQsyVi2RfDeefcW2K3rzvqmr8HrpCLcKDfojv3FujP2qORdFxrCALgKQ7MeEEJ7FNaA/A55WTECXZTTGCNG4xG+oq2QXywY0wrrG2H8EfOg2ic4kDPK5TSGIAx+GFPbLpLzjgilxy9gpvcpj5Ig0hXRJoiU80j8g4YwAWOGbL7GQlCL2X1BMbgB50j48r1Swbw6rfjq9/ZsO6uVUdGc/wz19FIAj3KfD9TaqR3ASGhkognHoYAAAAASUVORK5CYII=",
      "text/latex": [
       "$\\displaystyle \\frac{1}{2}$"
      ],
      "text/plain": [
       "1\n",
       "─\n",
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# puede cambiar el path por cualquier imagen que desee, ya sea de tamaño 150x180 o distinto\n",
    "# ya que se aplica un resize al obtener la matriz de pixeles\n",
    "image = dir+\"\\\\imagenes_evaluacion\\\\eval_1.png\"\n",
    "# se obtiene la matriz de pixeles que representan a la imagen de prueba\n",
    "img_val = tf.expand_dims(getPredImage(image),0)\n",
    "\n",
    "image2 = dir+\"\\\\imagenes_evaluacion\\\\eval_2.png\"\n",
    "img_val2 = tf.expand_dims(getPredImage(image2),0)\n",
    "\n",
    "# se establece la entrada del decoder como la etiqueta de inicio de una epxresion (4)\n",
    "input_decoder = tf.constant([4] * 1)\n",
    "# se se instancia una lista donde almacenar predicciones\n",
    "preds = []\n",
    "# se define el hidden state inicial del decoder como un tensor inicializado con ceros con las mismas\n",
    "# dimensiones que el feature map de salida del encoder\n",
    "hidden = tf.constant([[[0.0]*128]*3]*1)\n",
    "# se define el batch (no hay diferencia en utilizar la variable que guarda la matriz de la imagen, pero\n",
    "# se utiliza por consistencia)\n",
    "batch = img_val\n",
    "# se realiza la predccion con el encoder ingresando la imagen\n",
    "feature_map = encoder.predict(batch)\n",
    "\n",
    "init_printing()\n",
    "# se itera por la posible cantidad de etiquetas correctas a generar\n",
    "for i in range(90):\n",
    "    # se llama al decoder con el feature_map generado por en ecoder\n",
    "    # su entrada (etiqueta predicha anteriormnente) y su hidden state anterior\n",
    "    pred, hidden = decoder(feature_map, input_decoder, hidden)\n",
    "    # se toma como predicción el valor con mayor probabilidad\n",
    "    input_decoder = tf.argmax(pred, 1)\n",
    "    # se anade la prediccion a la lista de predicciones\n",
    "    preds.append(input_decoder)\n",
    "    # si se encuentra la etiqueta representativa del final de expresion de termina la iteracion\n",
    "    # ya que se habrá terminado con la predicción por parte del modelo.\n",
    "    if (input_decoder.numpy()[0] == 5):\n",
    "        break\n",
    "# se toman las llaves del diccionario de tokens\n",
    "keys = list(word_index.keys())\n",
    "# se imprime la secuencia de latex generada por las predicciones\n",
    "print(' '.join([str(list(word_index.keys())[i.numpy()[0]-1]  ) for i in preds]))\n",
    "render = ''\n",
    "# se añade en formato de texto la prediccion\n",
    "for i in preds:\n",
    "    print(i.numpy()[0], end=\" \")\n",
    "    render += keys[i.numpy()[0]-1] \n",
    "# la prediccion en formato de texto es formateada como latex e impresa\n",
    "render = parse_latex(str(render))\n",
    "render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys[96] = \"\\\\lt\"\n",
    "keys[105] = \"\\\\gt\"\n",
    "keys[14]='='\n",
    "keys[97] = '\\\\!'\n",
    "keys[39] = '\\\\,'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "_modelo_en_drive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
